{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Scrapy 爬虫框架学习教程\n",
        "\n",
        "## 目录\n",
        "1. [Scrapy简介](#scrapy简介)\n",
        "2. [环境准备](#环境准备)\n",
        "3. [第一个Scrapy爬虫](#第一个scrapy爬虫)\n",
        "4. [选择器（Selectors）详解](#选择器详解)\n",
        "5. [数据提取与Item](#数据提取与item)\n",
        "6. [请求与响应处理](#请求与响应处理)\n",
        "7. [爬虫进阶：中间件和管道](#爬虫进阶)\n",
        "8. [HTTP协议与网络安全](#http协议与网络安全)\n",
        "9. [合规爬虫实践](#合规爬虫实践)\n",
        "10. [实践练习](#实践练习)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Scrapy简介\n",
        "\n",
        "**Scrapy** 是一个用于爬取网站数据并提取结构化数据的Python框架。它是一个功能强大、可扩展的爬虫框架。\n",
        "\n",
        "### 计算机网络基础：爬虫工作原理\n",
        "\n",
        "想象一下，爬虫就像是一个**自动化的浏览器**：\n",
        "\n",
        "1. **发送HTTP请求**：爬虫向目标网站发送HTTP请求（就像你在浏览器地址栏输入网址）\n",
        "2. **接收HTTP响应**：服务器返回HTML、JSON等格式的数据（就像浏览器显示网页）\n",
        "3. **解析数据**：从响应中提取需要的信息（就像你阅读网页内容）\n",
        "4. **存储数据**：将提取的数据保存到文件或数据库\n",
        "\n",
        "### HTTP请求流程（形象化描述）\n",
        "\n",
        "```\n",
        "客户端（爬虫）                   服务器（网站）\n",
        "    |                                |\n",
        "    |--- HTTP请求 (GET/POST) ------->|\n",
        "    |   (包含URL、Headers等信息)      |\n",
        "    |                                |\n",
        "    |<-- HTTP响应 (200 OK) ----------|\n",
        "    |   (包含HTML内容、状态码等)      |\n",
        "    |                                |\n",
        "    |--- 解析HTML内容 --------------|\n",
        "    |--- 提取数据 ------------------|\n",
        "    |--- 存储数据 ------------------|\n",
        "```\n",
        "\n",
        "### Scrapy的主要特点\n",
        "- **高性能**：异步处理，可以并发爬取多个页面\n",
        "- **可扩展**：通过中间件和管道轻松扩展功能\n",
        "- **内置选择器**：支持CSS选择器和XPath\n",
        "- **自动去重**：自动处理重复请求\n",
        "- **遵守robots.txt**：可以配置遵守网站的爬虫协议\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 环境准备\n",
        "\n",
        "### 安装Scrapy\n",
        "\n",
        "在命令行中执行：\n",
        "```bash\n",
        "pip install scrapy\n",
        "```\n",
        "\n",
        "### 验证安装\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 导入scrapy库并查看版本\n",
        "import scrapy\n",
        "\n",
        "print(f\"Scrapy版本: {scrapy.__version__}\")\n",
        "\n",
        "# 检查scrapy命令是否可用（在命令行中运行：scrapy --version）\n",
        "# 注意：在Jupyter Notebook中，某些scrapy命令需要在终端执行\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 第一个Scrapy爬虫\n",
        "\n",
        "### 创建Scrapy项目（在命令行中执行）\n",
        "\n",
        "```bash\n",
        "# 创建项目\n",
        "scrapy startproject myproject\n",
        "\n",
        "# 进入项目目录\n",
        "cd myproject\n",
        "\n",
        "# 创建爬虫\n",
        "scrapy genspider example example.com\n",
        "```\n",
        "\n",
        "### 在Jupyter Notebook中直接使用Scrapy\n",
        "\n",
        "由于Jupyter Notebook的特殊环境，我们可以直接编写爬虫代码来演示：\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 示例1：最简单的Scrapy爬虫\n",
        "# 这个示例演示了如何创建一个基本的爬虫类\n",
        "\n",
        "from scrapy import Spider\n",
        "from scrapy.http import Request\n",
        "\n",
        "class SimpleSpider(Spider):\n",
        "    \"\"\"\n",
        "    Spider是Scrapy中所有爬虫的基类\n",
        "    \n",
        "    关键概念：\n",
        "    - name: 爬虫的唯一标识符，用于在命令行中运行爬虫\n",
        "    - start_urls: 爬虫开始爬取的URL列表\n",
        "    - parse(): 默认的回调函数，处理start_urls返回的响应\n",
        "    \"\"\"\n",
        "    name = 'simple'  # 爬虫名称，必须唯一\n",
        "    \n",
        "    # 起始URL列表，爬虫会从这些URL开始爬取\n",
        "    # 注意：这里使用httpbin.org作为示例，这是一个用于测试HTTP请求的网站\n",
        "    start_urls = [\n",
        "        'https://httpbin.org/html',  # 返回一个简单的HTML页面\n",
        "    ]\n",
        "    \n",
        "    def parse(self, response):\n",
        "        \"\"\"\n",
        "        parse方法是处理响应的回调函数\n",
        "        \n",
        "        参数：\n",
        "        - response: Response对象，包含服务器返回的HTML内容\n",
        "        \n",
        "        返回值：\n",
        "        - 可以返回Item对象（数据）、Request对象（继续爬取）或None\n",
        "        \"\"\"\n",
        "        # response.text 包含响应的文本内容（HTML）\n",
        "        # 这里我们只打印标题\n",
        "        title = response.css('title::text').get()\n",
        "        print(f\"页面标题: {title}\")\n",
        "        \n",
        "        # 返回None表示这个请求处理完成\n",
        "        return None\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 选择器（Selectors）详解\n",
        "\n",
        "Scrapy提供了强大的选择器来从HTML中提取数据，支持**CSS选择器**和**XPath**两种方式。\n",
        "\n",
        "### CSS选择器 vs XPath\n",
        "\n",
        "- **CSS选择器**：语法简洁，类似jQuery选择器，适合简单的选择\n",
        "- **XPath**：功能更强大，可以基于XML路径表达式选择元素，适合复杂场景\n",
        "\n",
        "### 选择器方法\n",
        "\n",
        "- `response.css('selector')` - 使用CSS选择器\n",
        "- `response.xpath('xpath')` - 使用XPath选择器\n",
        "- `.get()` - 获取第一个匹配的元素（返回字符串）\n",
        "- `.getall()` - 获取所有匹配的元素（返回列表）\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 示例2：使用选择器提取数据\n",
        "# 这个示例演示了CSS选择器和XPath的基本用法\n",
        "\n",
        "from scrapy import Spider\n",
        "from scrapy.selector import Selector\n",
        "\n",
        "# 创建一个示例HTML内容用于演示\n",
        "sample_html = \"\"\"\n",
        "<html>\n",
        "<head>\n",
        "    <title>示例网页</title>\n",
        "</head>\n",
        "<body>\n",
        "    <div class=\"container\">\n",
        "        <h1>欢迎学习Scrapy</h1>\n",
        "        <ul class=\"items\">\n",
        "            <li class=\"item\">项目1</li>\n",
        "            <li class=\"item\">项目2</li>\n",
        "            <li class=\"item\">项目3</li>\n",
        "        </ul>\n",
        "        <div id=\"content\">\n",
        "            <p>这是第一段内容</p>\n",
        "            <p>这是第二段内容</p>\n",
        "        </div>\n",
        "        <a href=\"https://example.com\">链接</a>\n",
        "    </div>\n",
        "</body>\n",
        "</html>\n",
        "\"\"\"\n",
        "\n",
        "# 创建Selector对象\n",
        "sel = Selector(text=sample_html)\n",
        "\n",
        "print(\"=== CSS选择器示例 ===\")\n",
        "# CSS选择器：选择title标签的文本内容\n",
        "title = sel.css('title::text').get()\n",
        "print(f\"标题: {title}\")\n",
        "\n",
        "# CSS选择器：选择所有class为item的li元素\n",
        "items = sel.css('li.item::text').getall()\n",
        "print(f\"项目列表: {items}\")\n",
        "\n",
        "# CSS选择器：选择id为content的div下的所有p标签\n",
        "paragraphs = sel.css('#content p::text').getall()\n",
        "print(f\"段落内容: {paragraphs}\")\n",
        "\n",
        "# CSS选择器：提取链接的href属性\n",
        "link = sel.css('a::attr(href)').get()\n",
        "print(f\"链接地址: {link}\")\n",
        "\n",
        "print(\"\\n=== XPath选择器示例 ===\")\n",
        "# XPath：选择title标签的文本内容\n",
        "title_xpath = sel.xpath('//title/text()').get()\n",
        "print(f\"标题(XPath): {title_xpath}\")\n",
        "\n",
        "# XPath：选择所有class为item的li元素\n",
        "items_xpath = sel.xpath('//li[@class=\"item\"]/text()').getall()\n",
        "print(f\"项目列表(XPath): {items_xpath}\")\n",
        "\n",
        "# XPath：选择id为content的div下的所有p标签\n",
        "paragraphs_xpath = sel.xpath('//div[@id=\"content\"]/p/text()').getall()\n",
        "print(f\"段落内容(XPath): {paragraphs_xpath}\")\n",
        "\n",
        "# XPath：提取链接的href属性\n",
        "link_xpath = sel.xpath('//a/@href').get()\n",
        "print(f\"链接地址(XPath): {link_xpath}\")\n",
        "\n",
        "print(\"\\n=== 选择器语法说明 ===\")\n",
        "print(\"\"\"\n",
        "CSS选择器语法：\n",
        "- 'tag' : 选择标签\n",
        "- '.class' : 选择class属性\n",
        "- '#id' : 选择id属性\n",
        "- '::text' : 提取文本内容\n",
        "- '::attr(attribute)' : 提取属性值\n",
        "\n",
        "XPath语法：\n",
        "- '//tag' : 选择所有tag标签（任意位置）\n",
        "- '/tag' : 选择直接子元素\n",
        "- '[@attribute=\"value\"]' : 属性选择\n",
        "- '/text()' : 提取文本内容\n",
        "- '/@attribute' : 提取属性值\n",
        "\"\"\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 数据提取与Item\n",
        "\n",
        "### Item：结构化数据容器\n",
        "\n",
        "在Scrapy中，**Item**用于定义要提取的数据结构。它类似于Python的字典，但提供了字段定义和验证功能。\n",
        "\n",
        "### Item的优势\n",
        "\n",
        "- **类型验证**：可以定义字段类型，自动验证数据\n",
        "- **代码提示**：IDE可以提供更好的代码补全\n",
        "- **文档化**：字段定义本身就是文档\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 示例3：使用Item定义数据结构\n",
        "\n",
        "from scrapy import Item, Field\n",
        "\n",
        "class BookItem(Item):\n",
        "    \"\"\"\n",
        "    Item类用于定义要提取的数据结构\n",
        "    \n",
        "    概念说明：\n",
        "    - Item类似于字典，但提供了字段定义\n",
        "    - Field()用于定义字段，可以设置元数据（如序列化器、验证器等）\n",
        "    - 字段名必须是字符串\n",
        "    \"\"\"\n",
        "    # 定义字段：字段名 = Field()\n",
        "    title = Field()      # 书名\n",
        "    author = Field()     # 作者\n",
        "    price = Field()      # 价格\n",
        "    url = Field()        # 链接\n",
        "    description = Field()  # 描述（可选字段）\n",
        "\n",
        "# 创建Item实例\n",
        "book = BookItem()\n",
        "book['title'] = 'Python编程'\n",
        "book['author'] = '作者名'\n",
        "book['price'] = '99.00'\n",
        "book['url'] = 'https://example.com/book1'\n",
        "\n",
        "print(\"Item内容:\")\n",
        "print(dict(book))  # 转换为字典查看\n",
        "\n",
        "# Item也可以像字典一样访问\n",
        "print(f\"\\n书名: {book['title']}\")\n",
        "print(f\"作者: {book.get('author', '未知')}\")  # 使用get方法，如果不存在返回默认值\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 示例4：完整的爬虫示例 - 提取数据到Item\n",
        "\n",
        "from scrapy import Spider, Item, Field\n",
        "from scrapy.http import Request\n",
        "\n",
        "# 定义Item（在实际项目中，通常在items.py文件中定义）\n",
        "class NewsItem(Item):\n",
        "    title = Field()\n",
        "    content = Field()\n",
        "    publish_date = Field()\n",
        "    author = Field()\n",
        "\n",
        "class NewsSpider(Spider):\n",
        "    \"\"\"\n",
        "    新闻爬虫示例\n",
        "    \n",
        "    这个爬虫演示了：\n",
        "    1. 如何从HTML中提取数据\n",
        "    2. 如何将数据填充到Item中\n",
        "    3. 如何返回Item对象\n",
        "    \"\"\"\n",
        "    name = 'news'\n",
        "    \n",
        "    # 注意：这里使用httpbin.org作为测试，实际项目中应该使用真实的新闻网站\n",
        "    # 合规提醒：爬取真实网站前，请务必：\n",
        "    # 1. 查看网站的robots.txt文件\n",
        "    # 2. 阅读网站的服务条款\n",
        "    # 3. 遵守网站的爬取频率限制\n",
        "    # 4. 不要对服务器造成过大压力\n",
        "    start_urls = [\n",
        "        'https://httpbin.org/html',\n",
        "    ]\n",
        "    \n",
        "    def parse(self, response):\n",
        "        \"\"\"\n",
        "        解析响应并提取数据\n",
        "        \n",
        "        工作流程：\n",
        "        1. 使用选择器从HTML中提取数据\n",
        "        2. 创建Item实例\n",
        "        3. 将提取的数据填充到Item中\n",
        "        4. 返回Item（Scrapy会自动处理Item，发送到Pipeline）\n",
        "        \"\"\"\n",
        "        # 创建Item实例\n",
        "        item = NewsItem()\n",
        "        \n",
        "        # 使用CSS选择器提取数据\n",
        "        # 注意：httpbin.org/html返回的页面结构简单，这里仅作演示\n",
        "        item['title'] = response.css('title::text').get() or '无标题'\n",
        "        item['content'] = response.css('h1::text').get() or '无内容'\n",
        "        \n",
        "        # 如果没有找到数据，可以设置默认值\n",
        "        item['publish_date'] = '2024-01-01'  # 示例数据\n",
        "        item['author'] = '未知作者'  # 示例数据\n",
        "        \n",
        "        # 返回Item，Scrapy会将Item发送到Pipeline进行处理\n",
        "        # 在Pipeline中可以保存到文件、数据库等\n",
        "        yield item\n",
        "        \n",
        "        # yield关键字说明：\n",
        "        # - yield是Python的生成器关键字\n",
        "        # - 在Scrapy中，yield可以返回Item或Request\n",
        "        # - 使用yield可以让Scrapy异步处理多个Item/Request\n",
        "        # - 相比return，yield不会立即结束函数，可以继续处理后续数据\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 请求与响应处理\n",
        "\n",
        "### 发送自定义请求\n",
        "\n",
        "在实际爬虫中，我们经常需要：\n",
        "- 发送POST请求（而不是默认的GET）\n",
        "- 添加自定义Headers（如User-Agent、Cookie等）\n",
        "- 传递参数（表单数据、JSON数据等）\n",
        "- 处理重定向、Cookie等\n",
        "\n",
        "### HTTP请求方法\n",
        "\n",
        "- **GET**：获取资源，参数在URL中（如：`?key=value`）\n",
        "- **POST**：提交数据，数据在请求体中（如：表单提交、API调用）\n",
        "- **HEAD**：只获取响应头，不获取响应体\n",
        "- **PUT/DELETE**：用于RESTful API\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 示例5：发送自定义请求\n",
        "\n",
        "from scrapy import Spider, Request\n",
        "from scrapy.http import FormRequest\n",
        "\n",
        "class CustomRequestSpider(Spider):\n",
        "    \"\"\"\n",
        "    演示如何发送自定义请求\n",
        "    \"\"\"\n",
        "    name = 'custom_request'\n",
        "    \n",
        "    def start_requests(self):\n",
        "        \"\"\"\n",
        "        重写start_requests方法，可以自定义初始请求\n",
        "        \n",
        "        与start_urls的区别：\n",
        "        - start_urls：简单场景，只能设置URL列表\n",
        "        - start_requests：复杂场景，可以设置Headers、Cookies、回调函数等\n",
        "        \"\"\"\n",
        "        # 方法1：使用Request对象，可以设置更多参数\n",
        "        yield Request(\n",
        "            url='https://httpbin.org/get',\n",
        "            method='GET',\n",
        "            headers={\n",
        "                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',\n",
        "                # User-Agent说明：\n",
        "                # - 标识客户端（浏览器/爬虫）的类型和版本\n",
        "                # - 有些网站会根据User-Agent返回不同的内容\n",
        "                # - 合规提醒：不要伪装成其他知名浏览器，保持真实身份\n",
        "            },\n",
        "            callback=self.parse_get,  # 指定处理响应的回调函数\n",
        "            meta={'page': 1}  # 传递额外的元数据\n",
        "        )\n",
        "        \n",
        "        # 方法2：发送POST请求（带表单数据）\n",
        "        yield FormRequest(\n",
        "            url='https://httpbin.org/post',\n",
        "            formdata={\n",
        "                'username': 'test',\n",
        "                'password': 'test123'\n",
        "            },\n",
        "            callback=self.parse_post,\n",
        "            # 合规提醒：不要爬取需要登录的网站，除非获得明确授权\n",
        "        )\n",
        "    \n",
        "    def parse_get(self, response):\n",
        "        \"\"\"\n",
        "        处理GET请求的响应\n",
        "        \"\"\"\n",
        "        # 访问meta中传递的数据\n",
        "        page = response.meta.get('page', 1)\n",
        "        print(f\"处理第 {page} 页\")\n",
        "        \n",
        "        # httpbin.org/get 会返回请求的信息（JSON格式）\n",
        "        import json\n",
        "        data = json.loads(response.text)\n",
        "        print(f\"请求URL: {data.get('url')}\")\n",
        "        print(f\"User-Agent: {data.get('headers', {}).get('User-Agent')}\")\n",
        "    \n",
        "    def parse_post(self, response):\n",
        "        \"\"\"\n",
        "        处理POST请求的响应\n",
        "        \"\"\"\n",
        "        import json\n",
        "        data = json.loads(response.text)\n",
        "        print(f\"POST数据: {data.get('form')}\")\n",
        "        \n",
        "        # 合规提醒：POST请求通常用于提交数据\n",
        "        # 不要向网站提交恶意数据或进行自动化攻击\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 示例6：处理分页和链接跟踪\n",
        "\n",
        "from scrapy import Spider, Request\n",
        "\n",
        "class PaginationSpider(Spider):\n",
        "    \"\"\"\n",
        "    演示如何处理分页和跟踪链接\n",
        "    \"\"\"\n",
        "    name = 'pagination'\n",
        "    \n",
        "    start_urls = ['https://httpbin.org/html']\n",
        "    \n",
        "    def parse(self, response):\n",
        "        \"\"\"\n",
        "        解析页面并跟踪链接\n",
        "        \"\"\"\n",
        "        # 提取当前页面的数据\n",
        "        title = response.css('title::text').get()\n",
        "        print(f\"当前页面: {title}\")\n",
        "        \n",
        "        # 示例：假设我们要跟踪页面中的链接\n",
        "        # 在实际场景中，可能是\"下一页\"链接或文章列表中的链接\n",
        "        links = response.css('a::attr(href)').getall()\n",
        "        \n",
        "        # 过滤和构建绝对URL\n",
        "        for link in links[:3]:  # 只处理前3个链接作为示例\n",
        "            if link:\n",
        "                # urljoin：将相对URL转换为绝对URL\n",
        "                # 例如：'/page2' + 'https://example.com/page1' = 'https://example.com/page2'\n",
        "                absolute_url = response.urljoin(link)\n",
        "                \n",
        "                # 创建新的Request，继续爬取\n",
        "                yield Request(\n",
        "                    url=absolute_url,\n",
        "                    callback=self.parse_detail,  # 使用不同的回调函数处理详情页\n",
        "                    # 注意：Scrapy会自动去重，相同的URL只会请求一次\n",
        "                )\n",
        "    \n",
        "    def parse_detail(self, response):\n",
        "        \"\"\"\n",
        "        处理详情页\n",
        "        \"\"\"\n",
        "        print(f\"处理详情页: {response.url}\")\n",
        "        \n",
        "        # 可以继续提取数据或跟踪更多链接\n",
        "        # 注意：要设置合理的深度限制，避免无限爬取\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 爬虫进阶：中间件和管道\n",
        "\n",
        "### Scrapy架构（形象化描述）\n",
        "\n",
        "```\n",
        "┌─────────────────────────────────────────┐\n",
        "│          Scrapy Engine (引擎)            │\n",
        "│  (协调所有组件，处理请求和响应)           │\n",
        "┌─────────────────────────────────────────┐\n",
        "│                                         │\n",
        "│  ┌──────────┐      ┌──────────┐        │\n",
        "│  │ Scheduler│ <──> │ Downloader│       │\n",
        "│  │ (调度器) │      │ (下载器) │        │\n",
        "│  └──────────┘      └──────────┘        │\n",
        "│       │                  │              │\n",
        "│       │                  ▼              │\n",
        "│       │         ┌─────────────────┐    │\n",
        "│       │         │ Downloader      │    │\n",
        "│       │         │ Middlewares     │    │\n",
        "│       │         │ (下载中间件)    │    │\n",
        "│       │         └─────────────────┘    │\n",
        "│       │                  │              │\n",
        "│       ▼                  ▼              │\n",
        "│  ┌──────────┐      ┌──────────┐        │\n",
        "│  │  Spider  │ ───> │ Response │        │\n",
        "│  │ (爬虫)   │      │ (响应)   │        │\n",
        "│  └──────────┘      └──────────┘        │\n",
        "│       │                                 │\n",
        "│       ▼                                 │\n",
        "│  ┌──────────┐                          │\n",
        "│  │   Item   │                          │\n",
        "│  └──────────┘                          │\n",
        "│       │                                 │\n",
        "│       ▼                                 │\n",
        "│  ┌─────────────────┐                   │\n",
        "│  │ Item Pipeline   │                   │\n",
        "│  │ (数据处理管道)   │                   │\n",
        "│  └─────────────────┘                   │\n",
        "└─────────────────────────────────────────┘\n",
        "```\n",
        "\n",
        "### 关键组件说明\n",
        "\n",
        "1. **Spider（爬虫）**：定义如何爬取网站，包括起始URL和解析逻辑\n",
        "2. **Scheduler（调度器）**：决定下一个要爬取的请求\n",
        "3. **Downloader（下载器）**：实际发送HTTP请求并获取响应\n",
        "4. **Middleware（中间件）**：在请求/响应处理过程中进行预处理\n",
        "5. **Pipeline（管道）**：处理Spider返回的Item（清洗、验证、存储）\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 示例7：自定义Pipeline处理数据\n",
        "\n",
        "from scrapy import Spider, Item, Field\n",
        "import json\n",
        "import csv\n",
        "\n",
        "# 定义Item\n",
        "class ProductItem(Item):\n",
        "    name = Field()\n",
        "    price = Field()\n",
        "    category = Field()\n",
        "\n",
        "# Pipeline示例1：保存为JSON文件\n",
        "class JsonPipeline:\n",
        "    \"\"\"\n",
        "    Pipeline用于处理Spider返回的Item\n",
        "    \n",
        "    工作流程：\n",
        "    1. process_item()：处理每个Item\n",
        "    2. open_spider()：爬虫开始时调用（初始化）\n",
        "    3. close_spider()：爬虫结束时调用（清理）\n",
        "    \"\"\"\n",
        "    \n",
        "    def open_spider(self, spider):\n",
        "        \"\"\"\n",
        "        爬虫开始时调用\n",
        "        用于初始化资源（如打开文件、连接数据库）\n",
        "        \"\"\"\n",
        "        self.file = open('products.json', 'w', encoding='utf-8')\n",
        "        self.items = []\n",
        "        print(\"Pipeline: 打开文件\")\n",
        "    \n",
        "    def close_spider(self, spider):\n",
        "        \"\"\"\n",
        "        爬虫结束时调用\n",
        "        用于清理资源（如关闭文件、断开数据库连接）\n",
        "        \"\"\"\n",
        "        json.dump(self.items, self.file, ensure_ascii=False, indent=2)\n",
        "        self.file.close()\n",
        "        print(f\"Pipeline: 保存了 {len(self.items)} 条数据到 products.json\")\n",
        "    \n",
        "    def process_item(self, item, spider):\n",
        "        \"\"\"\n",
        "        处理每个Item\n",
        "        \n",
        "        参数：\n",
        "        - item: Spider返回的Item对象\n",
        "        - spider: 当前运行的Spider实例\n",
        "        \n",
        "        返回值：\n",
        "        - 必须返回Item对象（或DropItem异常）\n",
        "        \"\"\"\n",
        "        # 将Item转换为字典并添加到列表\n",
        "        self.items.append(dict(item))\n",
        "        \n",
        "        # 可以在这里进行数据清洗、验证等操作\n",
        "        # 例如：去除空白字符、验证数据格式等\n",
        "        \n",
        "        return item  # 必须返回Item，否则后续Pipeline无法处理\n",
        "\n",
        "# Pipeline示例2：保存为CSV文件\n",
        "class CsvPipeline:\n",
        "    def open_spider(self, spider):\n",
        "        self.file = open('products.csv', 'w', newline='', encoding='utf-8')\n",
        "        self.writer = None\n",
        "        self.fieldnames = ['name', 'price', 'category']\n",
        "        print(\"CSV Pipeline: 打开文件\")\n",
        "    \n",
        "    def close_spider(self, spider):\n",
        "        if self.file:\n",
        "            self.file.close()\n",
        "        print(\"CSV Pipeline: 文件已关闭\")\n",
        "    \n",
        "    def process_item(self, item, spider):\n",
        "        if self.writer is None:\n",
        "            # 第一次调用时，创建CSV写入器并写入表头\n",
        "            self.writer = csv.DictWriter(self.file, fieldnames=self.fieldnames)\n",
        "            self.writer.writeheader()\n",
        "        \n",
        "        # 写入数据行\n",
        "        self.writer.writerow(dict(item))\n",
        "        return item\n",
        "\n",
        "# 使用Pipeline的Spider示例\n",
        "class ProductSpider(Spider):\n",
        "    name = 'product'\n",
        "    start_urls = ['https://httpbin.org/html']\n",
        "    \n",
        "    def parse(self, response):\n",
        "        # 创建示例Item\n",
        "        item = ProductItem()\n",
        "        item['name'] = '示例商品'\n",
        "        item['price'] = '99.00'\n",
        "        item['category'] = '电子产品'\n",
        "        \n",
        "        yield item  # Item会被发送到Pipeline处理\n",
        "\n",
        "# 注意：在实际项目中，Pipeline需要在settings.py中配置：\n",
        "# ITEM_PIPELINES = {\n",
        "#     'myproject.pipelines.JsonPipeline': 300,  # 数字表示优先级，越小越先执行\n",
        "#     'myproject.pipelines.CsvPipeline': 400,\n",
        "# }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## HTTP协议与网络安全\n",
        "\n",
        "### HTTP协议基础（形象化描述）\n",
        "\n",
        "HTTP（HyperText Transfer Protocol）就像**邮递员送信**的过程：\n",
        "\n",
        "```\n",
        "客户端（浏览器/爬虫）               服务器（网站）\n",
        "    |                                    |\n",
        "    |--- 1. 写请求信 ------------------->|\n",
        "    |     (请求行：GET /page HTTP/1.1)   |\n",
        "    |     (请求头：Host, User-Agent等)   |\n",
        "    |     (请求体：POST数据，如果有)     |\n",
        "    |                                    |\n",
        "    |<-- 2. 收到回信 --------------------|\n",
        "    |     (状态行：HTTP/1.1 200 OK)      |\n",
        "    |     (响应头：Content-Type等)       |\n",
        "    |     (响应体：HTML/JSON内容)        |\n",
        "    |                                    |\n",
        "```\n",
        "\n",
        "### HTTP状态码（重要概念）\n",
        "\n",
        "- **200 OK**：请求成功\n",
        "- **301/302**：重定向（网站搬家了，告诉你新地址）\n",
        "- **404 Not Found**：页面不存在\n",
        "- **403 Forbidden**：禁止访问（可能被反爬虫机制拦截）\n",
        "- **500 Internal Server Error**：服务器错误\n",
        "- **503 Service Unavailable**：服务不可用（可能服务器过载）\n",
        "\n",
        "### HTTP请求头（Headers）详解\n",
        "\n",
        "请求头就像**信封上的信息**，告诉服务器关于请求的详细信息：\n",
        "\n",
        "- **User-Agent**：客户端标识（浏览器类型、操作系统等）\n",
        "- **Referer**：来源页面（从哪个页面跳转过来的）\n",
        "- **Cookie**：会话信息（登录状态、购物车等）\n",
        "- **Accept**：客户端能接受的内容类型\n",
        "- **Content-Type**：请求体的内容类型（POST请求时使用）\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 示例8：理解HTTP请求和响应\n",
        "\n",
        "from scrapy.http import Request, Response\n",
        "from scrapy import Spider\n",
        "\n",
        "class HttpUnderstandingSpider(Spider):\n",
        "    \"\"\"\n",
        "    演示HTTP请求和响应的各个组成部分\n",
        "    \"\"\"\n",
        "    name = 'http_demo'\n",
        "    \n",
        "    def start_requests(self):\n",
        "        # 创建一个详细的请求\n",
        "        request = Request(\n",
        "            url='https://httpbin.org/headers',  # httpbin.org可以返回请求的详细信息\n",
        "            headers={\n",
        "                'User-Agent': 'MyScrapyBot/1.0',\n",
        "                'Accept': 'text/html,application/json',\n",
        "                'Referer': 'https://example.com',\n",
        "            }\n",
        "        )\n",
        "        \n",
        "        print(\"=== HTTP请求信息 ===\")\n",
        "        print(f\"请求方法: {request.method}\")\n",
        "        print(f\"请求URL: {request.url}\")\n",
        "        print(f\"请求头: {dict(request.headers)}\")\n",
        "        print(f\"请求体: {request.body}\")\n",
        "        \n",
        "        yield request\n",
        "    \n",
        "    def parse(self, response):\n",
        "        print(\"\\n=== HTTP响应信息 ===\")\n",
        "        print(f\"状态码: {response.status}\")\n",
        "        print(f\"响应URL: {response.url}\")\n",
        "        print(f\"响应头: {dict(response.headers)}\")\n",
        "        print(f\"响应体长度: {len(response.body)} 字节\")\n",
        "        \n",
        "        # httpbin.org/headers 会返回我们发送的请求头\n",
        "        import json\n",
        "        try:\n",
        "            data = json.loads(response.text)\n",
        "            print(f\"\\n服务器收到的请求头:\")\n",
        "            print(json.dumps(data.get('headers', {}), indent=2, ensure_ascii=False))\n",
        "        except:\n",
        "            print(\"响应内容:\", response.text[:200])  # 只显示前200个字符\n",
        "        \n",
        "        # 状态码处理示例\n",
        "        if response.status == 200:\n",
        "            print(\"\\n✓ 请求成功\")\n",
        "        elif response.status == 404:\n",
        "            print(\"\\n✗ 页面不存在\")\n",
        "        elif response.status == 403:\n",
        "            print(\"\\n✗ 访问被禁止（可能触发了反爬虫机制）\")\n",
        "        elif response.status >= 500:\n",
        "            print(\"\\n✗ 服务器错误\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 网络安全问题：反爬虫机制\n",
        "\n",
        "网站为了保护自己，会使用各种**反爬虫机制**：\n",
        "\n",
        "#### 1. User-Agent检测\n",
        "- **原理**：检查请求头中的User-Agent\n",
        "- **应对**：设置合理的User-Agent（但不要伪装成知名浏览器）\n",
        "\n",
        "#### 2. 频率限制（Rate Limiting）\n",
        "- **原理**：限制同一IP的请求频率\n",
        "- **形象化**：就像银行ATM，同一张卡短时间内只能取款有限次数\n",
        "- **应对**：设置下载延迟（DOWNLOAD_DELAY）\n",
        "\n",
        "#### 3. IP封禁\n",
        "- **原理**：检测到异常行为后封禁IP地址\n",
        "- **应对**：使用代理IP（但需要遵守代理服务商的使用条款）\n",
        "\n",
        "#### 4. Cookie/Session验证\n",
        "- **原理**：需要登录或验证身份才能访问\n",
        "- **合规提醒**：不要尝试绕过登录验证，这通常违反服务条款\n",
        "\n",
        "#### 5. JavaScript渲染\n",
        "- **原理**：内容通过JavaScript动态加载\n",
        "- **应对**：使用Selenium或Splash（但会增加复杂度）\n",
        "\n",
        "#### 6. 验证码（CAPTCHA）\n",
        "- **原理**：要求人类验证\n",
        "- **合规提醒**：不要使用自动化工具绕过验证码\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 示例9：设置合理的爬取参数（避免被封禁）\n",
        "\n",
        "from scrapy import Spider\n",
        "from scrapy.settings import Settings\n",
        "\n",
        "class PoliteSpider(Spider):\n",
        "    \"\"\"\n",
        "    演示如何设置合理的爬取参数，做一个\"有礼貌\"的爬虫\n",
        "    \"\"\"\n",
        "    name = 'polite'\n",
        "    \n",
        "    # 自定义设置\n",
        "    custom_settings = {\n",
        "        # 下载延迟：每次请求之间等待1-3秒（随机）\n",
        "        # 形象化：就像人类浏览网页，不会瞬间点击所有链接\n",
        "        'DOWNLOAD_DELAY': 2,  # 固定延迟2秒\n",
        "        'RANDOMIZE_DOWNLOAD_DELAY': True,  # 随机延迟（0.5 * DOWNLOAD_DELAY 到 1.5 * DOWNLOAD_DELAY）\n",
        "        \n",
        "        # 并发请求数：同时进行的请求数量\n",
        "        # 形象化：就像同时打开多个浏览器标签页\n",
        "        'CONCURRENT_REQUESTS': 16,  # 默认16个\n",
        "        'CONCURRENT_REQUESTS_PER_DOMAIN': 8,  # 每个域名最多8个并发\n",
        "        \n",
        "        # User-Agent：设置合理的用户代理\n",
        "        'USER_AGENT': 'MyScrapyBot/1.0 (+https://example.com/bot-info)',\n",
        "        # 合规提醒：提供bot信息页面，说明爬虫的目的和联系方式\n",
        "        \n",
        "        # 遵守robots.txt\n",
        "        'ROBOTSTXT_OBEY': True,\n",
        "        # robots.txt说明：\n",
        "        # - 网站根目录下的文件，告诉爬虫哪些可以爬，哪些不能爬\n",
        "        # - 例如：User-agent: *  Disallow: /admin/\n",
        "        # - 遵守robots.txt是网络爬虫的基本礼仪\n",
        "        \n",
        "        # 自动重试\n",
        "        'RETRY_ENABLED': True,\n",
        "        'RETRY_TIMES': 3,  # 失败后重试3次\n",
        "        'RETRY_HTTP_CODES': [500, 502, 503, 504, 408, 429],  # 这些状态码会重试\n",
        "        \n",
        "        # 超时设置\n",
        "        'DOWNLOAD_TIMEOUT': 180,  # 180秒超时\n",
        "    }\n",
        "    \n",
        "    start_urls = ['https://httpbin.org/delay/1']  # 模拟延迟1秒的请求\n",
        "    \n",
        "    def parse(self, response):\n",
        "        print(\"请求成功完成\")\n",
        "        print(f\"响应时间: {response.meta.get('download_latency', 0):.2f} 秒\")\n",
        "        \n",
        "        # 合规提醒总结：\n",
        "        # 1. 设置合理的延迟，不要对服务器造成压力\n",
        "        # 2. 遵守robots.txt协议\n",
        "        # 3. 设置合理的User-Agent，不要伪装\n",
        "        # 4. 不要绕过网站的访问限制\n",
        "        # 5. 尊重网站的服务条款\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 合规爬虫实践\n",
        "\n",
        "### 法律和道德考量\n",
        "\n",
        "网络爬虫虽然技术上是可行的，但必须遵守**法律法规**和**道德规范**：\n",
        "\n",
        "#### 1. 遵守robots.txt协议\n",
        "\n",
        "**robots.txt**是网站告诉爬虫的\"交通规则\"：\n",
        "\n",
        "```\n",
        "User-agent: *          # 适用于所有爬虫\n",
        "Allow: /public/        # 允许爬取/public/目录\n",
        "Disallow: /private/    # 禁止爬取/private/目录\n",
        "Disallow: /admin/      # 禁止爬取/admin/目录\n",
        "Crawl-delay: 10         # 请求间隔至少10秒\n",
        "```\n",
        "\n",
        "**合规做法**：\n",
        "- 在Scrapy中设置 `ROBOTSTXT_OBEY = True`\n",
        "- 遵守网站设置的爬取规则\n",
        "\n",
        "#### 2. 遵守服务条款（Terms of Service）\n",
        "\n",
        "**重要**：大多数网站的服务条款都禁止：\n",
        "- 自动化访问（除非明确允许）\n",
        "- 大规模数据抓取\n",
        "- 商业用途的数据使用\n",
        "\n",
        "**合规做法**：\n",
        "- 爬取前阅读网站的服务条款\n",
        "- 如有疑问，联系网站管理员获得授权\n",
        "- 仅爬取公开数据，不爬取需要登录的内容\n",
        "\n",
        "#### 3. 数据使用限制\n",
        "\n",
        "**合规做法**：\n",
        "- **个人学习/研究**：通常可以接受\n",
        "- **商业用途**：需要获得明确授权\n",
        "- **个人隐私数据**：严格禁止（如用户个人信息、联系方式等）\n",
        "- **版权内容**：遵守版权法，不要大规模复制受版权保护的内容\n",
        "\n",
        "#### 4. 技术合规\n",
        "\n",
        "**合规做法**：\n",
        "- 设置合理的请求频率（不要DDoS攻击）\n",
        "- 不要绕过安全措施（如验证码、登录验证）\n",
        "- 不要伪装成其他服务（如伪装成Googlebot）\n",
        "- 提供联系方式（在User-Agent中）\n",
        "\n",
        "#### 5. 数据存储和使用\n",
        "\n",
        "**合规做法**：\n",
        "- 仅存储必要的数据\n",
        "- 遵守数据保护法规（如GDPR）\n",
        "- 不要将数据用于非法用途\n",
        "- 定期清理不需要的数据\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 示例10：合规爬虫的最佳实践\n",
        "\n",
        "from scrapy import Spider, Request\n",
        "from scrapy.utils.response import get_base_url\n",
        "\n",
        "class CompliantSpider(Spider):\n",
        "    \"\"\"\n",
        "    演示合规爬虫的最佳实践\n",
        "    \"\"\"\n",
        "    name = 'compliant'\n",
        "    \n",
        "    custom_settings = {\n",
        "        # 1. 遵守robots.txt\n",
        "        'ROBOTSTXT_OBEY': True,\n",
        "        \n",
        "        # 2. 设置合理的延迟\n",
        "        'DOWNLOAD_DELAY': 2,\n",
        "        'RANDOMIZE_DOWNLOAD_DELAY': True,\n",
        "        \n",
        "        # 3. 设置合理的User-Agent（包含联系方式）\n",
        "        'USER_AGENT': 'MyResearchBot/1.0 (Contact: researcher@example.com; Purpose: Academic Research)',\n",
        "        \n",
        "        # 4. 限制并发数\n",
        "        'CONCURRENT_REQUESTS_PER_DOMAIN': 2,\n",
        "        \n",
        "        # 5. 设置超时\n",
        "        'DOWNLOAD_TIMEOUT': 30,\n",
        "        \n",
        "        # 6. 自动重试（但不要无限重试）\n",
        "        'RETRY_ENABLED': True,\n",
        "        'RETRY_TIMES': 2,\n",
        "    }\n",
        "    \n",
        "    def start_requests(self):\n",
        "        \"\"\"\n",
        "        在发送请求前，可以进行合规性检查\n",
        "        \"\"\"\n",
        "        # 示例：检查是否应该爬取某个URL\n",
        "        url = 'https://httpbin.org/html'\n",
        "        \n",
        "        # 合规检查示例：\n",
        "        # 1. 检查URL是否在允许的域名列表中\n",
        "        # 2. 检查是否需要登录（如果需要，应该停止）\n",
        "        # 3. 检查是否违反了robots.txt\n",
        "        \n",
        "        yield Request(\n",
        "            url=url,\n",
        "            callback=self.parse,\n",
        "            errback=self.handle_error,  # 错误处理回调\n",
        "            dont_filter=False,  # 使用默认的去重机制\n",
        "        )\n",
        "    \n",
        "    def parse(self, response):\n",
        "        \"\"\"\n",
        "        解析响应\n",
        "        \"\"\"\n",
        "        # 合规检查：检查响应状态\n",
        "        if response.status == 403:\n",
        "            self.logger.warning(\"收到403错误，可能触发了反爬虫机制，建议停止爬取\")\n",
        "            return\n",
        "        \n",
        "        if response.status == 429:\n",
        "            self.logger.warning(\"收到429错误（请求过多），建议增加延迟时间\")\n",
        "            return\n",
        "        \n",
        "        # 提取数据\n",
        "        title = response.css('title::text').get()\n",
        "        print(f\"页面标题: {title}\")\n",
        "        \n",
        "        # 合规提醒：\n",
        "        # - 只提取公开可见的数据\n",
        "        # - 不要提取个人隐私信息\n",
        "        # - 不要提取受版权保护的内容\n",
        "    \n",
        "    def handle_error(self, failure):\n",
        "        \"\"\"\n",
        "        错误处理\n",
        "        \"\"\"\n",
        "        # 记录错误，但不要无限重试\n",
        "        self.logger.error(f\"请求失败: {failure.value}\")\n",
        "        \n",
        "        # 合规做法：遇到错误时，应该：\n",
        "        # 1. 记录错误日志\n",
        "        # 2. 不要立即重试（避免对服务器造成压力）\n",
        "        # 3. 分析错误原因（是否是合规问题）\n",
        "\n",
        "# 合规检查清单函数\n",
        "def compliance_checklist():\n",
        "    \"\"\"\n",
        "    爬虫开发前的合规检查清单\n",
        "    \"\"\"\n",
        "    checklist = \"\"\"\n",
        "    ✓ 合规爬虫检查清单：\n",
        "    \n",
        "    1. 法律合规\n",
        "       [ ] 阅读并理解目标网站的服务条款\n",
        "       [ ] 确认爬取行为不违反法律法规\n",
        "       [ ] 确认数据使用目的合法\n",
        "    \n",
        "    2. 技术合规\n",
        "       [ ] 设置ROBOTSTXT_OBEY = True\n",
        "       [ ] 设置合理的DOWNLOAD_DELAY\n",
        "       [ ] 设置合理的User-Agent（包含联系方式）\n",
        "       [ ] 限制并发请求数\n",
        "       [ ] 实现错误处理和日志记录\n",
        "    \n",
        "    3. 数据合规\n",
        "       [ ] 仅爬取公开数据\n",
        "       [ ] 不爬取个人隐私信息\n",
        "       [ ] 不爬取需要登录的内容（除非获得授权）\n",
        "       [ ] 遵守数据保护法规\n",
        "    \n",
        "    4. 道德合规\n",
        "       [ ] 不对服务器造成过大压力\n",
        "       [ ] 尊重网站的反爬虫机制\n",
        "       [ ] 提供明确的爬虫身份和目的\n",
        "       [ ] 在遇到问题时主动联系网站管理员\n",
        "    \"\"\"\n",
        "    print(checklist)\n",
        "\n",
        "# 运行检查清单\n",
        "compliance_checklist()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 实践练习\n",
        "\n",
        "### 练习1：基础爬虫\n",
        "\n",
        "创建一个爬虫，从 httpbin.org/html 提取页面标题和所有链接。\n",
        "\n",
        "### 练习2：数据提取\n",
        "\n",
        "创建一个Item类，包含以下字段：\n",
        "- title（标题）\n",
        "- links（链接列表）\n",
        "- text_content（文本内容）\n",
        "\n",
        "### 练习3：处理多个页面\n",
        "\n",
        "创建一个爬虫，能够：\n",
        "1. 从起始页面提取链接\n",
        "2. 跟踪链接到详情页\n",
        "3. 从详情页提取数据\n",
        "\n",
        "### 练习4：数据存储\n",
        "\n",
        "创建一个Pipeline，将数据保存为：\n",
        "- JSON格式\n",
        "- CSV格式\n",
        "\n",
        "### 练习5：错误处理\n",
        "\n",
        "实现错误处理逻辑：\n",
        "- 处理404错误\n",
        "- 处理403错误\n",
        "- 处理超时错误\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 练习1和2的示例代码\n",
        "\n",
        "from scrapy import Spider, Item, Field\n",
        "\n",
        "# 定义Item\n",
        "class PageItem(Item):\n",
        "    title = Field()\n",
        "    links = Field()  # 列表类型\n",
        "    text_content = Field()\n",
        "\n",
        "class PracticeSpider(Spider):\n",
        "    \"\"\"\n",
        "    练习：基础爬虫和数据提取\n",
        "    \"\"\"\n",
        "    name = 'practice'\n",
        "    start_urls = ['https://httpbin.org/html']\n",
        "    \n",
        "    def parse(self, response):\n",
        "        # 创建Item\n",
        "        item = PageItem()\n",
        "        \n",
        "        # 提取标题\n",
        "        item['title'] = response.css('title::text').get()\n",
        "        \n",
        "        # 提取所有链接（使用getall获取列表）\n",
        "        item['links'] = response.css('a::attr(href)').getall()\n",
        "        \n",
        "        # 提取文本内容（去除HTML标签）\n",
        "        # 注意：这里只是简单示例，实际可能需要更复杂的处理\n",
        "        item['text_content'] = ' '.join(response.css('body *::text').getall())\n",
        "        \n",
        "        # 打印结果\n",
        "        print(f\"标题: {item['title']}\")\n",
        "        print(f\"链接数量: {len(item['links'])}\")\n",
        "        print(f\"文本内容长度: {len(item['text_content'])} 字符\")\n",
        "        \n",
        "        return item\n",
        "\n",
        "# 注意：在实际运行中，需要使用scrapy crawl命令：\n",
        "# scrapy crawl practice -o output.json\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 总结\n",
        "\n",
        "### 关键知识点回顾\n",
        "\n",
        "1. **Scrapy架构**：理解Spider、Scheduler、Downloader、Pipeline等组件的作用\n",
        "2. **选择器**：掌握CSS选择器和XPath的基本用法\n",
        "3. **Item**：使用Item定义数据结构，提高代码可维护性\n",
        "4. **请求处理**：理解HTTP请求和响应的各个组成部分\n",
        "5. **Pipeline**：使用Pipeline处理、清洗和存储数据\n",
        "6. **合规实践**：遵守robots.txt、设置合理延迟、尊重网站规则\n",
        "\n",
        "### 进一步学习方向\n",
        "\n",
        "- **Scrapy官方文档**：https://docs.scrapy.org/\n",
        "- **XPath教程**：学习更复杂的XPath表达式\n",
        "- **中间件开发**：自定义下载中间件和爬虫中间件\n",
        "- **分布式爬虫**：使用Scrapy-Redis实现分布式爬取\n",
        "- **JavaScript渲染**：使用Splash处理动态内容\n",
        "\n",
        "### 重要提醒\n",
        "\n",
        "⚠️ **合规第一**：在开发任何爬虫之前，务必：\n",
        "1. 阅读目标网站的服务条款\n",
        "2. 检查robots.txt文件\n",
        "3. 设置合理的爬取参数\n",
        "4. 尊重网站的反爬虫机制\n",
        "5. 仅用于合法目的\n",
        "\n",
        "🔒 **安全注意**：\n",
        "- 不要爬取需要登录的内容（除非获得授权）\n",
        "- 不要绕过安全验证（如验证码）\n",
        "- 不要对服务器造成过大压力\n",
        "- 保护爬取到的数据，遵守隐私法规\n",
        "\n",
        "---\n",
        "\n",
        "**祝学习愉快！记住：做一个有礼貌、合规的爬虫开发者！** 🕷️\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
