{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BeautifulSoup 和 lxml 库学习指南\n",
    "\n",
    "## 目录\n",
    "1. [库简介与安装](#库简介与安装)\n",
    "2. [HTML基础与解析原理](#html基础与解析原理)\n",
    "3. [BeautifulSoup基础操作](#beautifulsoup基础操作)\n",
    "4. [元素查找方法](#元素查找方法)\n",
    "5. [数据提取与清洗](#数据提取与清洗)\n",
    "6. [lxml解析器详解](#lxml解析器详解)\n",
    "7. [HTTP协议基础](#http协议基础)\n",
    "8. [网络爬虫安全与合规](#网络爬虫安全与合规)\n",
    "9. [实战案例](#实战案例)\n",
    "10. [常见问题与最佳实践](#常见问题与最佳实践)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 库简介与安装\n",
    "\n",
    "### BeautifulSoup 简介\n",
    "**BeautifulSoup** 是一个Python库，用于从HTML和XML文档中提取数据。它提供了简单直观的API来解析、搜索和修改解析树。\n",
    "\n",
    "### lxml 简介\n",
    "**lxml** 是一个高性能的XML和HTML解析库，BeautifulSoup可以使用lxml作为解析器后端，提供更快的解析速度。\n",
    "\n",
    "### 主要特点：\n",
    "- **简单易用**：直观的API设计，适合初学者\n",
    "- **灵活强大**：支持多种解析器（html.parser, lxml, html5lib）\n",
    "- **容错性强**：能够处理格式不规范的HTML文档\n",
    "- **高效解析**：使用lxml解析器时性能优异\n",
    "\n",
    "### 安装方法：\n",
    "```bash\n",
    "pip install beautifulsoup4 lxml\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BeautifulSoup 和 lxml 库导入成功！\n"
     ]
    }
   ],
   "source": [
    "# 导入必要的库\n",
    "from bs4 import BeautifulSoup\n",
    "import requests  # 用于发送HTTP请求（后续会用到）\n",
    "\n",
    "# 检查安装是否成功\n",
    "print(\"BeautifulSoup 和 lxml 库导入成功！\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HTML基础与解析原理\n",
    "\n",
    "### HTML结构简介\n",
    "HTML（超文本标记语言）是网页的基础结构，由标签（tags）、属性（attributes）和内容（content）组成。\n",
    "\n",
    "**形象化理解**：\n",
    "- **HTML文档** = 一棵树（DOM树）\n",
    "- **标签** = 树的节点\n",
    "- **属性** = 节点的属性（如id、class等）\n",
    "- **内容** = 节点中的文本\n",
    "\n",
    "### 解析过程\n",
    "1. **输入**：HTML字符串或文件\n",
    "2. **解析**：解析器将HTML转换为树状结构\n",
    "3. **遍历**：通过BeautifulSoup API访问和操作节点\n",
    "4. **提取**：获取所需的数据\n",
    "\n",
    "### 示例HTML结构\n",
    "```html\n",
    "<html>\n",
    "  <head>\n",
    "    <title>示例页面</title>\n",
    "  </head>\n",
    "  <body>\n",
    "    <div class=\"content\">\n",
    "      <h1>标题</h1>\n",
    "      <p>段落内容</p>\n",
    "    </div>\n",
    "  </body>\n",
    "</html>\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "解析后的对象类型: <class 'bs4.BeautifulSoup'>\n",
      "\n",
      "美化后的HTML输出:\n",
      "<html>\n",
      " <head>\n",
      "  <title>\n",
      "   我的第一个网页\n",
      "  </title>\n",
      " </head>\n",
      " <body>\n",
      "  <h1>\n",
      "   欢迎学习BeautifulSoup\n",
      "  </h1>\n",
      "  <p class=\"intro\">\n",
      "   这是一个介绍段落\n",
      "  </p>\n",
      "  <p>\n",
      "   这是另一个段落\n",
      "  </p>\n",
      " </body>\n",
      "</html>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 示例1：创建简单的HTML字符串并解析\n",
    "# 这是最基础的用法，从字符串创建BeautifulSoup对象\n",
    "\n",
    "html_string = \"\"\"\n",
    "<html>\n",
    "<head>\n",
    "    <title>我的第一个网页</title>\n",
    "</head>\n",
    "<body>\n",
    "    <h1>欢迎学习BeautifulSoup</h1>\n",
    "    <p class=\"intro\">这是一个介绍段落</p>\n",
    "    <p>这是另一个段落</p>\n",
    "</body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "# 使用BeautifulSoup解析HTML\n",
    "# 第一个参数：要解析的HTML字符串\n",
    "# 第二个参数：指定解析器，'lxml'表示使用lxml解析器（速度快）\n",
    "soup = BeautifulSoup(html_string, 'lxml')\n",
    "\n",
    "# 打印解析后的对象类型\n",
    "print(\"解析后的对象类型:\", type(soup))\n",
    "print(\"\\n美化后的HTML输出:\")\n",
    "print(soup.prettify())  # prettify()方法可以格式化输出HTML，使其更易读\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BeautifulSoup基础操作\n",
    "\n",
    "### 访问元素的方法\n",
    "1. **点号访问**：通过标签名直接访问（只返回第一个匹配的元素）\n",
    "2. **find()方法**：查找单个元素\n",
    "3. **find_all()方法**：查找所有匹配的元素\n",
    "\n",
    "### 重要概念\n",
    "- **Tag对象**：代表HTML标签\n",
    "- **NavigableString**：代表标签内的文本内容\n",
    "- **BeautifulSoup对象**：代表整个文档\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 示例2：访问HTML元素的基础操作\n",
    "\n",
    "html_doc = \"\"\"\n",
    "<html>\n",
    "<body>\n",
    "    <div id=\"main\">\n",
    "        <h1>产品列表</h1>\n",
    "        <ul class=\"products\">\n",
    "            <li class=\"item\" data-price=\"99\">商品A</li>\n",
    "            <li class=\"item\" data-price=\"199\">商品B</li>\n",
    "            <li class=\"item\" data-price=\"299\">商品C</li>\n",
    "        </ul>\n",
    "    </div>\n",
    "</body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "soup = BeautifulSoup(html_doc, 'lxml')\n",
    "\n",
    "# 方法1：使用点号访问标签（返回第一个匹配的元素）\n",
    "# 注意：如果标签名是Python关键字（如class），需要使用下划线\n",
    "title = soup.h1  # 获取第一个h1标签\n",
    "print(\"方法1 - 点号访问:\")\n",
    "print(f\"标题内容: {title.string}\")  # .string获取标签内的文本\n",
    "print(f\"标题标签名: {title.name}\")  # .name获取标签名\n",
    "print()\n",
    "\n",
    "# 方法2：使用find()方法查找单个元素\n",
    "# find()返回第一个匹配的元素，如果没找到返回None\n",
    "first_item = soup.find('li')\n",
    "print(\"方法2 - find()方法:\")\n",
    "print(f\"第一个商品: {first_item.string}\")\n",
    "print(f\"商品价格属性: {first_item.get('data-price')}\")  # get()方法获取属性值\n",
    "print()\n",
    "\n",
    "# 方法3：通过属性查找元素\n",
    "# 可以传入属性字典来查找特定属性的元素\n",
    "main_div = soup.find('div', {'id': 'main'})\n",
    "print(\"方法3 - 通过属性查找:\")\n",
    "print(f\"找到的div: {main_div.name}\")\n",
    "print()\n",
    "\n",
    "# 方法4：使用class查找（注意：class是Python关键字，所以用class_）\n",
    "# 注意：class在HTML中是常见属性，BeautifulSoup使用class_来避免冲突\n",
    "items = soup.find_all('li', class_='item')\n",
    "print(\"方法4 - 查找所有class='item'的元素:\")\n",
    "for i, item in enumerate(items, 1):\n",
    "    price = item.get('data-price')\n",
    "    print(f\"商品{i}: {item.string}, 价格: {price}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 元素查找方法\n",
    "\n",
    "### 常用查找方法对比\n",
    "\n",
    "| 方法 | 返回结果 | 使用场景 |\n",
    "|------|---------|---------|\n",
    "| `find()` | 单个元素或None | 查找唯一元素 |\n",
    "| `find_all()` | 元素列表 | 查找多个元素 |\n",
    "| `select()` | 元素列表 | 使用CSS选择器 |\n",
    "| `select_one()` | 单个元素或None | CSS选择器查找单个元素 |\n",
    "\n",
    "### CSS选择器基础\n",
    "CSS选择器是一种强大的查找方式，类似于在浏览器中查找元素：\n",
    "- `#id`：通过id查找\n",
    "- `.class`：通过class查找\n",
    "- `tag`：通过标签名查找\n",
    "- `tag.class`：组合查找\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 示例3：使用find_all()查找多个元素\n",
    "\n",
    "html_table = \"\"\"\n",
    "<table class=\"data-table\">\n",
    "    <thead>\n",
    "        <tr>\n",
    "            <th>姓名</th>\n",
    "            <th>年龄</th>\n",
    "            <th>城市</th>\n",
    "        </tr>\n",
    "    </thead>\n",
    "    <tbody>\n",
    "        <tr>\n",
    "            <td>张三</td>\n",
    "            <td>25</td>\n",
    "            <td>北京</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>李四</td>\n",
    "            <td>30</td>\n",
    "            <td>上海</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>王五</td>\n",
    "            <td>28</td>\n",
    "            <td>广州</td>\n",
    "        </tr>\n",
    "    </tbody>\n",
    "</table>\n",
    "\"\"\"\n",
    "\n",
    "soup = BeautifulSoup(html_table, 'lxml')\n",
    "\n",
    "# 查找所有tr标签（表格行）\n",
    "rows = soup.find_all('tr')\n",
    "print(\"所有表格行:\")\n",
    "for i, row in enumerate(rows):\n",
    "    # 获取行内所有td或th标签\n",
    "    cells = row.find_all(['td', 'th'])  # 可以传入列表查找多种标签\n",
    "    cell_texts = [cell.get_text(strip=True) for cell in cells]  # get_text()获取文本，strip=True去除空白\n",
    "    print(f\"第{i+1}行: {cell_texts}\")\n",
    "print()\n",
    "\n",
    "# 查找tbody中的所有行（限制查找范围）\n",
    "tbody = soup.find('tbody')\n",
    "tbody_rows = tbody.find_all('tr')\n",
    "print(\"tbody中的数据行:\")\n",
    "for row in tbody_rows:\n",
    "    cells = row.find_all('td')\n",
    "    data = [cell.get_text(strip=True) for cell in cells]\n",
    "    print(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 示例4：使用CSS选择器（更强大的查找方式）\n",
    "\n",
    "html_complex = \"\"\"\n",
    "<div class=\"container\">\n",
    "    <div class=\"product\" id=\"prod1\">\n",
    "        <h2 class=\"title\">笔记本电脑</h2>\n",
    "        <span class=\"price\">5999元</span>\n",
    "        <p class=\"description\">高性能处理器</p>\n",
    "    </div>\n",
    "    <div class=\"product\" id=\"prod2\">\n",
    "        <h2 class=\"title\">智能手机</h2>\n",
    "        <span class=\"price\">3999元</span>\n",
    "        <p class=\"description\">大屏幕显示</p>\n",
    "    </div>\n",
    "    <div class=\"product special\" id=\"prod3\">\n",
    "        <h2 class=\"title\">平板电脑</h2>\n",
    "        <span class=\"price\">2999元</span>\n",
    "        <p class=\"description\">轻薄便携</p>\n",
    "    </div>\n",
    "</div>\n",
    "\"\"\"\n",
    "\n",
    "soup = BeautifulSoup(html_complex, 'lxml')\n",
    "\n",
    "# CSS选择器方法1：通过class查找\n",
    "# .product 表示class为product的元素\n",
    "products = soup.select('.product')\n",
    "print(\"方法1 - 通过class选择器:\")\n",
    "print(f\"找到{len(products)}个商品\")\n",
    "print()\n",
    "\n",
    "# CSS选择器方法2：通过id查找\n",
    "# #prod1 表示id为prod1的元素\n",
    "product1 = soup.select_one('#prod1')\n",
    "print(\"方法2 - 通过id选择器:\")\n",
    "print(f\"商品1的标题: {product1.select_one('.title').get_text()}\")\n",
    "print()\n",
    "\n",
    "# CSS选择器方法3：组合选择器\n",
    "# .product .price 表示在.product元素内查找.price元素\n",
    "prices = soup.select('.product .price')\n",
    "print(\"方法3 - 组合选择器查找价格:\")\n",
    "for price in prices:\n",
    "    print(price.get_text())\n",
    "print()\n",
    "\n",
    "# CSS选择器方法4：属性选择器\n",
    "# [id^=\"prod\"] 表示id以\"prod\"开头的元素\n",
    "products_with_id = soup.select('[id^=\"prod\"]')\n",
    "print(\"方法4 - 属性选择器:\")\n",
    "print(f\"找到{len(products_with_id)}个带id的商品\")\n",
    "print()\n",
    "\n",
    "# 提取所有商品信息到列表\n",
    "product_list = []\n",
    "for product in soup.select('.product'):\n",
    "    title = product.select_one('.title').get_text()\n",
    "    price = product.select_one('.price').get_text()\n",
    "    desc = product.select_one('.description').get_text()\n",
    "    product_list.append({\n",
    "        'title': title,\n",
    "        'price': price,\n",
    "        'description': desc\n",
    "    })\n",
    "\n",
    "print(\"提取的商品信息:\")\n",
    "for item in product_list:\n",
    "    print(item)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据提取与清洗\n",
    "\n",
    "### 文本提取方法\n",
    "- `get_text()`：获取标签及其子标签的所有文本\n",
    "- `.string`：获取单个字符串（仅当标签只有一个子节点时）\n",
    "- `.strings`：获取所有子标签的文本生成器\n",
    "- `stripped_strings`：获取去除空白后的文本生成器\n",
    "\n",
    "### 属性提取\n",
    "- `tag['attribute']`：直接访问属性（可能抛出KeyError）\n",
    "- `tag.get('attribute')`：安全获取属性（不存在返回None）\n",
    "- `tag.get('attribute', 'default')`：带默认值的获取\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 示例5：数据提取与清洗技巧\n",
    "\n",
    "html_messy = \"\"\"\n",
    "<div class=\"article\">\n",
    "    <h1>   Python编程指南  </h1>\n",
    "    <p>\n",
    "        这是一段包含<strong>重要</strong>信息的文本。\n",
    "        还有<em>强调</em>的内容。\n",
    "    </p>\n",
    "    <ul>\n",
    "        <li>项目1</li>\n",
    "        <li>项目2</li>\n",
    "        <li>项目3</li>\n",
    "    </ul>\n",
    "    <a href=\"https://example.com\" class=\"link\" data-id=\"123\">访问链接</a>\n",
    "</div>\n",
    "\"\"\"\n",
    "\n",
    "soup = BeautifulSoup(html_messy, 'lxml')\n",
    "\n",
    "# 方法1：get_text() - 获取所有文本内容\n",
    "# separator参数可以指定分隔符，strip=True去除首尾空白\n",
    "article = soup.find('div', class_='article')\n",
    "full_text = article.get_text(separator=' ', strip=True)\n",
    "print(\"方法1 - get_text()提取所有文本:\")\n",
    "print(full_text)\n",
    "print()\n",
    "\n",
    "# 方法2：提取特定标签的文本\n",
    "title = soup.h1.get_text(strip=True)  # 去除首尾空白\n",
    "print(\"方法2 - 提取标题:\")\n",
    "print(f\"标题: '{title}'\")\n",
    "print()\n",
    "\n",
    "# 方法3：提取链接的href属性\n",
    "link = soup.find('a')\n",
    "# 方法3a：直接访问（如果属性不存在会报错）\n",
    "href1 = link['href']\n",
    "print(\"方法3a - 直接访问属性:\")\n",
    "print(f\"链接地址: {href1}\")\n",
    "\n",
    "# 方法3b：使用get()方法（更安全，推荐）\n",
    "href2 = link.get('href')\n",
    "data_id = link.get('data-id')\n",
    "print(\"方法3b - 使用get()方法:\")\n",
    "print(f\"链接地址: {href2}, 数据ID: {data_id}\")\n",
    "\n",
    "# 方法3c：带默认值的get()\n",
    "class_name = link.get('class', 'no-class')  # 如果class不存在，返回'no-class'\n",
    "print(f\"class属性: {class_name}\")\n",
    "print()\n",
    "\n",
    "# 方法4：提取列表项\n",
    "items = soup.find_all('li')\n",
    "item_texts = [item.get_text(strip=True) for item in items]\n",
    "print(\"方法4 - 提取列表项:\")\n",
    "print(item_texts)\n",
    "print()\n",
    "\n",
    "# 方法5：处理嵌套标签\n",
    "# 如果只想获取直接文本，不包含子标签文本，需要特殊处理\n",
    "p_tag = soup.find('p')\n",
    "# 获取所有直接文本节点（不包含子标签）\n",
    "direct_texts = []\n",
    "for element in p_tag.children:\n",
    "    if hasattr(element, 'string') and element.string:\n",
    "        direct_texts.append(element.string.strip())\n",
    "print(\"方法5 - 提取直接文本（不含子标签）:\")\n",
    "print(direct_texts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 示例6：创建数据表格（综合应用）\n",
    "\n",
    "# 模拟从网页提取的数据\n",
    "html_data = \"\"\"\n",
    "<div class=\"student-list\">\n",
    "    <div class=\"student\" data-id=\"1\">\n",
    "        <span class=\"name\">张三</span>\n",
    "        <span class=\"age\">20</span>\n",
    "        <span class=\"score\">85</span>\n",
    "        <span class=\"major\">计算机科学</span>\n",
    "    </div>\n",
    "    <div class=\"student\" data-id=\"2\">\n",
    "        <span class=\"name\">李四</span>\n",
    "        <span class=\"age\">21</span>\n",
    "        <span class=\"score\">92</span>\n",
    "        <span class=\"major\">数学</span>\n",
    "    </div>\n",
    "    <div class=\"student\" data-id=\"3\">\n",
    "        <span class=\"name\">王五</span>\n",
    "        <span class=\"age\">19</span>\n",
    "        <span class=\"score\">78</span>\n",
    "        <span class=\"major\">物理</span>\n",
    "    </div>\n",
    "    <div class=\"student\" data-id=\"4\">\n",
    "        <span class=\"name\">赵六</span>\n",
    "        <span class=\"age\">22</span>\n",
    "        <span class=\"score\">95</span>\n",
    "        <span class=\"major\">计算机科学</span>\n",
    "    </div>\n",
    "</div>\n",
    "\"\"\"\n",
    "\n",
    "soup = BeautifulSoup(html_data, 'lxml')\n",
    "\n",
    "# 提取所有学生信息\n",
    "students = []\n",
    "for student_div in soup.select('.student'):\n",
    "    # 使用字典存储每个学生的信息\n",
    "    student_info = {\n",
    "        'id': student_div.get('data-id'),\n",
    "        'name': student_div.select_one('.name').get_text(strip=True),\n",
    "        'age': int(student_div.select_one('.age').get_text(strip=True)),  # 转换为整数\n",
    "        'score': int(student_div.select_one('.score').get_text(strip=True)),\n",
    "        'major': student_div.select_one('.major').get_text(strip=True)\n",
    "    }\n",
    "    students.append(student_info)\n",
    "\n",
    "# 打印表格形式的数据\n",
    "print(\"学生信息表:\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"{'ID':<5} {'姓名':<10} {'年龄':<5} {'分数':<5} {'专业':<15}\")\n",
    "print(\"-\" * 60)\n",
    "for student in students:\n",
    "    print(f\"{student['id']:<5} {student['name']:<10} {student['age']:<5} {student['score']:<5} {student['major']:<15}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# 计算统计数据\n",
    "total_students = len(students)\n",
    "avg_score = sum(s['score'] for s in students) / total_students\n",
    "max_score = max(s['score'] for s in students)\n",
    "min_score = min(s['score'] for s in students)\n",
    "\n",
    "print(f\"\\n统计信息:\")\n",
    "print(f\"总学生数: {total_students}\")\n",
    "print(f\"平均分数: {avg_score:.2f}\")\n",
    "print(f\"最高分: {max_score}\")\n",
    "print(f\"最低分: {min_score}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## lxml解析器详解\n",
    "\n",
    "### 解析器对比\n",
    "\n",
    "| 解析器 | 速度 | 容错性 | 依赖 | 适用场景 |\n",
    "|--------|------|--------|------|---------|\n",
    "| **lxml** | 快 | 中等 | 需要安装lxml | 生产环境推荐 |\n",
    "| **html.parser** | 中等 | 好 | Python内置 | 无需额外依赖 |\n",
    "| **html5lib** | 慢 | 最好 | 需要安装html5lib | 处理不规范HTML |\n",
    "\n",
    "### lxml的优势\n",
    "- **性能优异**：C语言实现，解析速度快\n",
    "- **标准兼容**：支持XML和HTML标准\n",
    "- **功能强大**：支持XPath（需要额外配置）\n",
    "\n",
    "### 使用建议\n",
    "- **开发环境**：可以使用html.parser（无需安装）\n",
    "- **生产环境**：推荐使用lxml（性能更好）\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 示例7：不同解析器的使用和对比\n",
    "\n",
    "html_sample = \"\"\"\n",
    "<html>\n",
    "<body>\n",
    "    <p>测试内容</p>\n",
    "    <div>另一个元素</div>\n",
    "</body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "# 使用lxml解析器（推荐，速度快）\n",
    "soup_lxml = BeautifulSoup(html_sample, 'lxml')\n",
    "print(\"使用lxml解析器:\")\n",
    "print(f\"找到的p标签: {soup_lxml.find('p').get_text()}\")\n",
    "print()\n",
    "\n",
    "# 使用html.parser（Python内置，无需额外安装）\n",
    "soup_parser = BeautifulSoup(html_sample, 'html.parser')\n",
    "print(\"使用html.parser解析器:\")\n",
    "print(f\"找到的p标签: {soup_parser.find('p').get_text()}\")\n",
    "print()\n",
    "\n",
    "# 注意：如果HTML格式不规范，不同解析器的处理结果可能不同\n",
    "# 例如，处理未闭合的标签时，lxml会自动修复，html.parser也会尝试修复\n",
    "bad_html = \"<p>未闭合的段落<div>另一个元素\"\n",
    "soup_bad = BeautifulSoup(bad_html, 'lxml')\n",
    "print(\"处理不规范HTML（lxml会自动修复）:\")\n",
    "print(soup_bad.prettify())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HTTP协议基础\n",
    "\n",
    "### HTTP请求过程（形象化理解）\n",
    "\n",
    "想象你在餐厅点餐：\n",
    "1. **客户端（浏览器/爬虫）** = 顾客\n",
    "2. **服务器** = 餐厅\n",
    "3. **HTTP请求** = 点餐单\n",
    "4. **HTTP响应** = 上菜\n",
    "\n",
    "### HTTP请求的组成部分\n",
    "\n",
    "```\n",
    "请求行：GET /index.html HTTP/1.1\n",
    "请求头：\n",
    "  User-Agent: 浏览器标识（告诉服务器\"我是谁\"）\n",
    "  Accept: 接受的内容类型\n",
    "  Cookie: 身份验证信息\n",
    "请求体：（GET请求通常没有，POST请求有数据）\n",
    "```\n",
    "\n",
    "### HTTP响应码\n",
    "\n",
    "| 状态码 | 含义 | 形象化描述 |\n",
    "|--------|------|-----------|\n",
    "| 200 | 成功 | 菜品上齐，一切正常 |\n",
    "| 404 | 未找到 | 餐厅没有这道菜 |\n",
    "| 403 | 禁止访问 | 餐厅不接待你 |\n",
    "| 500 | 服务器错误 | 餐厅厨房出问题了 |\n",
    "\n",
    "### 重要HTTP头信息\n",
    "\n",
    "- **User-Agent**：标识客户端类型（浏览器、爬虫等）\n",
    "- **Referer**：来源页面\n",
    "- **Cookie**：会话信息\n",
    "- **Accept**：客户端接受的内容类型\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 示例8：发送HTTP请求并解析响应（模拟网络爬虫）\n",
    "\n",
    "# 注意：以下代码仅用于教学演示\n",
    "# 在实际使用中，必须遵守网站的robots.txt和使用条款\n",
    "\n",
    "# 使用requests库发送HTTP请求\n",
    "# 这是一个常用的HTTP库，用于获取网页内容\n",
    "\n",
    "# 示例：解析本地HTML（安全的方式，不涉及网络请求）\n",
    "# 在实际爬虫中，你会使用 requests.get(url) 获取网页内容\n",
    "\n",
    "# 模拟一个网页响应\n",
    "html_response = \"\"\"\n",
    "<!DOCTYPE html>\n",
    "<html lang=\"zh-CN\">\n",
    "<head>\n",
    "    <meta charset=\"UTF-8\">\n",
    "    <title>新闻列表</title>\n",
    "</head>\n",
    "<body>\n",
    "    <div class=\"news-container\">\n",
    "        <article class=\"news-item\">\n",
    "            <h2><a href=\"/news/1\">Python 3.12 正式发布</a></h2>\n",
    "            <p class=\"date\">2024-01-15</p>\n",
    "            <p class=\"summary\">Python 3.12带来了性能提升和新特性...</p>\n",
    "        </article>\n",
    "        <article class=\"news-item\">\n",
    "            <h2><a href=\"/news/2\">BeautifulSoup 4.12 更新</a></h2>\n",
    "            <p class=\"date\">2024-01-20</p>\n",
    "            <p class=\"summary\">新版本改进了解析性能和错误处理...</p>\n",
    "        </article>\n",
    "    </div>\n",
    "</body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "# 解析HTML响应\n",
    "soup = BeautifulSoup(html_response, 'lxml')\n",
    "\n",
    "# 提取新闻信息\n",
    "news_list = []\n",
    "for article in soup.select('.news-item'):\n",
    "    title_tag = article.select_one('h2 a')\n",
    "    title = title_tag.get_text(strip=True)\n",
    "    link = title_tag.get('href')\n",
    "    date = article.select_one('.date').get_text(strip=True)\n",
    "    summary = article.select_one('.summary').get_text(strip=True)\n",
    "    \n",
    "    news_list.append({\n",
    "        'title': title,\n",
    "        'link': link,\n",
    "        'date': date,\n",
    "        'summary': summary\n",
    "    })\n",
    "\n",
    "# 显示提取的新闻\n",
    "print(\"新闻列表:\")\n",
    "print(\"=\" * 80)\n",
    "for i, news in enumerate(news_list, 1):\n",
    "    print(f\"\\n新闻 {i}:\")\n",
    "    print(f\"  标题: {news['title']}\")\n",
    "    print(f\"  链接: {news['link']}\")\n",
    "    print(f\"  日期: {news['date']}\")\n",
    "    print(f\"  摘要: {news['summary']}\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "# 注意：如果要从真实网站获取数据，代码应该是这样的：\n",
    "# import requests\n",
    "# \n",
    "# # 设置请求头，模拟浏览器访问\n",
    "# headers = {\n",
    "#     'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n",
    "# }\n",
    "# \n",
    "# # 发送GET请求\n",
    "# response = requests.get('https://example.com', headers=headers)\n",
    "# \n",
    "# # 检查响应状态码\n",
    "# if response.status_code == 200:\n",
    "#     soup = BeautifulSoup(response.text, 'lxml')\n",
    "#     # 解析数据...\n",
    "# else:\n",
    "#     print(f\"请求失败，状态码: {response.status_code}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 网络爬虫安全与合规\n",
    "\n",
    "### ⚠️ 重要合规问题说明\n",
    "\n",
    "#### 1. robots.txt 协议\n",
    "- **什么是robots.txt**：网站根目录下的文件，告诉爬虫哪些可以爬，哪些不可以\n",
    "- **如何遵守**：在爬取前检查 `https://网站域名/robots.txt`\n",
    "- **形象化理解**：就像进入图书馆前看\"阅览须知\"\n",
    "\n",
    "#### 2. 网站使用条款（Terms of Service）\n",
    "- **必须遵守**：每个网站都有使用条款，违反可能面临法律风险\n",
    "- **常见限制**：禁止自动化访问、禁止商业用途等\n",
    "- **建议**：爬取前仔细阅读网站的使用条款\n",
    "\n",
    "#### 3. 请求频率控制\n",
    "- **问题**：过于频繁的请求会加重服务器负担，可能被视为攻击\n",
    "- **解决方案**：\n",
    "  - 添加延时（time.sleep()）\n",
    "  - 限制并发数\n",
    "  - 使用代理池（高级用法）\n",
    "\n",
    "#### 4. User-Agent 设置\n",
    "- **为什么重要**：有些网站会拒绝没有User-Agent或User-Agent异常的请求\n",
    "- **正确做法**：设置合理的User-Agent，模拟真实浏览器\n",
    "- **合规说明**：虽然设置User-Agent是技术手段，但不意味着可以绕过网站限制\n",
    "\n",
    "#### 5. 数据使用合规\n",
    "- **个人隐私**：不要爬取和存储个人敏感信息\n",
    "- **版权问题**：爬取的内容可能受版权保护，使用需谨慎\n",
    "- **商业用途**：商业使用爬取数据前，确保获得授权\n",
    "\n",
    "### 网络安全注意事项\n",
    "\n",
    "#### 1. HTTPS vs HTTP\n",
    "- **HTTPS**：加密传输，更安全（推荐）\n",
    "- **HTTP**：明文传输，可能被窃听（不推荐用于敏感数据）\n",
    "\n",
    "#### 2. 验证SSL证书\n",
    "- **默认行为**：requests库默认验证SSL证书\n",
    "- **危险操作**：`verify=False` 会跳过证书验证（仅用于测试，生产环境禁用）\n",
    "\n",
    "#### 3. 处理敏感信息\n",
    "- **不要硬编码**：API密钥、密码等不要写在代码中\n",
    "- **使用环境变量**：敏感信息存储在环境变量中\n",
    "- **不要提交到Git**：确保.gitignore包含敏感文件\n",
    "\n",
    "### 最佳实践清单\n",
    "\n",
    "✅ **应该做的**：\n",
    "- 遵守robots.txt\n",
    "- 设置合理的请求间隔\n",
    "- 使用适当的User-Agent\n",
    "- 处理异常和错误\n",
    "- 尊重网站服务器资源\n",
    "- 仅爬取公开数据\n",
    "\n",
    "❌ **不应该做的**：\n",
    "- 绕过网站安全措施\n",
    "- 过度频繁请求\n",
    "- 爬取需要登录的私人数据（未授权）\n",
    "- 商业使用未授权数据\n",
    "- 忽略网站使用条款\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 示例9：安全的HTTP请求实践（仅演示，不实际请求）\n",
    "\n",
    "import time  # 用于添加延时\n",
    "\n",
    "# ============================================================================\n",
    "# 合规的爬虫代码示例（模板）\n",
    "# ============================================================================\n",
    "\n",
    "def safe_web_scraping_example():\n",
    "    \"\"\"\n",
    "    这是一个展示如何安全、合规地进行网页爬取的示例函数\n",
    "    \n",
    "    重要提醒：\n",
    "    1. 在实际使用前，必须检查目标网站的robots.txt\n",
    "    2. 必须遵守网站的使用条款（Terms of Service）\n",
    "    3. 不要过度频繁地请求，避免给服务器造成负担\n",
    "    4. 仅用于学习和合法的数据获取\n",
    "    \"\"\"\n",
    "    \n",
    "    # 步骤1：设置合理的请求头\n",
    "    # User-Agent告诉服务器你是什么类型的客户端\n",
    "    # 使用真实的浏览器User-Agent是常见做法，但要注意：\n",
    "    # - 这不意味着可以绕过网站的限制\n",
    "    # - 必须遵守网站的使用条款\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
    "        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n",
    "        'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',\n",
    "        # 注意：Referer表示来源页面，某些网站会检查这个\n",
    "        # 'Referer': 'https://example.com'\n",
    "    }\n",
    "    \n",
    "    # 步骤2：检查robots.txt（在实际代码中应该实现）\n",
    "    # robots_url = 'https://example.com/robots.txt'\n",
    "    # 应该使用urllib.robotparser模块解析robots.txt\n",
    "    # 这里仅作说明，不实际实现\n",
    "    \n",
    "    # 步骤3：发送请求（这里仅演示，不实际发送）\n",
    "    # url = 'https://example.com'\n",
    "    # response = requests.get(url, headers=headers, timeout=10)\n",
    "    # \n",
    "    # # 检查响应状态码\n",
    "    # if response.status_code == 200:\n",
    "    #     soup = BeautifulSoup(response.text, 'lxml')\n",
    "    #     # 解析数据...\n",
    "    # else:\n",
    "    #     print(f\"请求失败: {response.status_code}\")\n",
    "    \n",
    "    # 步骤4：添加延时（重要！）\n",
    "    # 在循环请求多个页面时，必须添加延时\n",
    "    # 建议延时：1-3秒，根据网站负载调整\n",
    "    # time.sleep(2)  # 延时2秒\n",
    "    \n",
    "    print(\"安全爬虫代码模板已展示\")\n",
    "    print(\"\\n重要提醒：\")\n",
    "    print(\"1. 实际使用前必须检查robots.txt\")\n",
    "    print(\"2. 必须遵守网站使用条款\")\n",
    "    print(\"3. 添加适当的请求延时\")\n",
    "    print(\"4. 不要过度频繁请求\")\n",
    "    print(\"5. 仅用于合法目的\")\n",
    "\n",
    "# 调用示例函数\n",
    "safe_web_scraping_example()\n",
    "\n",
    "# ============================================================================\n",
    "# 错误处理示例\n",
    "# ============================================================================\n",
    "\n",
    "def robust_scraping_example():\n",
    "    \"\"\"\n",
    "    展示如何处理网络请求中的各种异常情况\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"错误处理示例：\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # 在实际代码中，应该这样处理：\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers, timeout=10)\n",
    "        response.raise_for_status()  # 如果状态码不是200，抛出异常\n",
    "        soup = BeautifulSoup(response.text, 'lxml')\n",
    "        # 处理数据...\n",
    "    except requests.exceptions.Timeout:\n",
    "        print(\"请求超时\")\n",
    "    except requests.exceptions.ConnectionError:\n",
    "        print(\"连接错误\")\n",
    "    except requests.exceptions.HTTPError as e:\n",
    "        print(f\"HTTP错误: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"其他错误: {e}\")\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"在实际代码中应该包含完整的异常处理\")\n",
    "\n",
    "robust_scraping_example()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 实战案例\n",
    "\n",
    "### 案例1：解析本地HTML文件\n",
    "在实际项目中，你可能需要解析保存的HTML文件。\n",
    "\n",
    "### 案例2：提取表格数据\n",
    "很多网站使用表格展示数据，学会提取表格数据很重要。\n",
    "\n",
    "### 案例3：处理动态内容\n",
    "注意：BeautifulSoup只能解析静态HTML，对于JavaScript动态加载的内容，需要使用Selenium等工具。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 实战案例1：解析复杂的HTML结构并提取结构化数据\n",
    "\n",
    "html_complex = \"\"\"\n",
    "<!DOCTYPE html>\n",
    "<html>\n",
    "<head>\n",
    "    <meta charset=\"UTF-8\">\n",
    "    <title>电商产品页面</title>\n",
    "</head>\n",
    "<body>\n",
    "    <div class=\"product-detail\">\n",
    "        <h1 class=\"product-title\">iPhone 15 Pro Max</h1>\n",
    "        <div class=\"product-info\">\n",
    "            <span class=\"price\">¥8999</span>\n",
    "            <span class=\"original-price\">¥9999</span>\n",
    "            <span class=\"discount\">9折</span>\n",
    "        </div>\n",
    "        <div class=\"specifications\">\n",
    "            <table>\n",
    "                <tr>\n",
    "                    <th>规格</th>\n",
    "                    <th>详情</th>\n",
    "                </tr>\n",
    "                <tr>\n",
    "                    <td>屏幕尺寸</td>\n",
    "                    <td>6.7英寸</td>\n",
    "                </tr>\n",
    "                <tr>\n",
    "                    <td>存储容量</td>\n",
    "                    <td>256GB</td>\n",
    "                </tr>\n",
    "                <tr>\n",
    "                    <td>颜色</td>\n",
    "                    <td>深空黑色</td>\n",
    "                </tr>\n",
    "            </table>\n",
    "        </div>\n",
    "        <div class=\"reviews\">\n",
    "            <div class=\"review-item\">\n",
    "                <span class=\"user\">用户A</span>\n",
    "                <span class=\"rating\">5星</span>\n",
    "                <p class=\"comment\">非常好用，推荐购买！</p>\n",
    "            </div>\n",
    "            <div class=\"review-item\">\n",
    "                <span class=\"user\">用户B</span>\n",
    "                <span class=\"rating\">4星</span>\n",
    "                <p class=\"comment\">整体不错，就是价格有点高。</p>\n",
    "            </div>\n",
    "        </div>\n",
    "    </div>\n",
    "</body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "soup = BeautifulSoup(html_complex, 'lxml')\n",
    "\n",
    "# 提取产品基本信息\n",
    "product_data = {\n",
    "    'title': soup.select_one('.product-title').get_text(strip=True),\n",
    "    'current_price': soup.select_one('.price').get_text(strip=True),\n",
    "    'original_price': soup.select_one('.original-price').get_text(strip=True),\n",
    "    'discount': soup.select_one('.discount').get_text(strip=True)\n",
    "}\n",
    "\n",
    "print(\"产品基本信息:\")\n",
    "print(f\"  标题: {product_data['title']}\")\n",
    "print(f\"  现价: {product_data['current_price']}\")\n",
    "print(f\"  原价: {product_data['original_price']}\")\n",
    "print(f\"  折扣: {product_data['discount']}\")\n",
    "print()\n",
    "\n",
    "# 提取规格表格数据\n",
    "specs = {}\n",
    "spec_table = soup.select('.specifications table tr')[1:]  # 跳过表头\n",
    "for row in spec_table:\n",
    "    cells = row.find_all('td')\n",
    "    if len(cells) == 2:\n",
    "        key = cells[0].get_text(strip=True)\n",
    "        value = cells[1].get_text(strip=True)\n",
    "        specs[key] = value\n",
    "\n",
    "print(\"产品规格:\")\n",
    "for key, value in specs.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "print()\n",
    "\n",
    "# 提取用户评价\n",
    "reviews = []\n",
    "for review in soup.select('.review-item'):\n",
    "    user = review.select_one('.user').get_text(strip=True)\n",
    "    rating = review.select_one('.rating').get_text(strip=True)\n",
    "    comment = review.select_one('.comment').get_text(strip=True)\n",
    "    reviews.append({\n",
    "        'user': user,\n",
    "        'rating': rating,\n",
    "        'comment': comment\n",
    "    })\n",
    "\n",
    "print(\"用户评价:\")\n",
    "for i, review in enumerate(reviews, 1):\n",
    "    print(f\"  评价{i}:\")\n",
    "    print(f\"    用户: {review['user']}\")\n",
    "    print(f\"    评分: {review['rating']}\")\n",
    "    print(f\"    评论: {review['comment']}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 实战案例2：处理嵌套表格和复杂数据结构\n",
    "\n",
    "html_nested = \"\"\"\n",
    "<div class=\"data-container\">\n",
    "    <h2>销售报表</h2>\n",
    "    <table class=\"sales-table\">\n",
    "        <thead>\n",
    "            <tr>\n",
    "                <th>月份</th>\n",
    "                <th>产品A</th>\n",
    "                <th>产品B</th>\n",
    "                <th>产品C</th>\n",
    "                <th>总计</th>\n",
    "            </tr>\n",
    "        </thead>\n",
    "        <tbody>\n",
    "            <tr>\n",
    "                <td>1月</td>\n",
    "                <td>1000</td>\n",
    "                <td>1500</td>\n",
    "                <td>800</td>\n",
    "                <td>3300</td>\n",
    "            </tr>\n",
    "            <tr>\n",
    "                <td>2月</td>\n",
    "                <td>1200</td>\n",
    "                <td>1600</td>\n",
    "                <td>900</td>\n",
    "                <td>3700</td>\n",
    "            </tr>\n",
    "            <tr>\n",
    "                <td>3月</td>\n",
    "                <td>1100</td>\n",
    "                <td>1400</td>\n",
    "                <td>850</td>\n",
    "                <td>3350</td>\n",
    "            </tr>\n",
    "        </tbody>\n",
    "    </table>\n",
    "</div>\n",
    "\"\"\"\n",
    "\n",
    "soup = BeautifulSoup(html_nested, 'lxml')\n",
    "\n",
    "# 提取表头\n",
    "table = soup.find('table', class_='sales-table')\n",
    "headers = [th.get_text(strip=True) for th in table.find('thead').find_all('th')]\n",
    "print(\"表头:\", headers)\n",
    "print()\n",
    "\n",
    "# 提取表格数据\n",
    "rows_data = []\n",
    "tbody = table.find('tbody')\n",
    "for row in tbody.find_all('tr'):\n",
    "    cells = row.find_all('td')\n",
    "    row_data = {\n",
    "        headers[0]: cells[0].get_text(strip=True),  # 月份\n",
    "        headers[1]: int(cells[1].get_text(strip=True)),  # 产品A\n",
    "        headers[2]: int(cells[2].get_text(strip=True)),  # 产品B\n",
    "        headers[3]: int(cells[3].get_text(strip=True)),  # 产品C\n",
    "        headers[4]: int(cells[4].get_text(strip=True))   # 总计\n",
    "    }\n",
    "    rows_data.append(row_data)\n",
    "\n",
    "# 打印表格数据\n",
    "print(\"表格数据:\")\n",
    "print(\"-\" * 70)\n",
    "header_line = f\"{headers[0]:<8} {headers[1]:<10} {headers[2]:<10} {headers[3]:<10} {headers[4]:<10}\"\n",
    "print(header_line)\n",
    "print(\"-\" * 70)\n",
    "for row in rows_data:\n",
    "    print(f\"{row[headers[0]]:<8} {row[headers[1]]:<10} {row[headers[2]]:<10} {row[headers[3]]:<10} {row[headers[4]]:<10}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "# 计算统计数据\n",
    "print(\"\\n统计分析:\")\n",
    "total_a = sum(row[headers[1]] for row in rows_data)\n",
    "total_b = sum(row[headers[2]] for row in rows_data)\n",
    "total_c = sum(row[headers[3]] for row in rows_data)\n",
    "grand_total = sum(row[headers[4]] for row in rows_data)\n",
    "\n",
    "print(f\"{headers[1]} 总计: {total_a}\")\n",
    "print(f\"{headers[2]} 总计: {total_b}\")\n",
    "print(f\"{headers[3]} 总计: {total_c}\")\n",
    "print(f\"所有产品总计: {grand_total}\")\n",
    "print(f\"平均每月销售额: {grand_total / len(rows_data):.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 常见问题与最佳实践\n",
    "\n",
    "### 常见问题\n",
    "\n",
    "#### 1. 为什么找不到元素？\n",
    "- **可能原因**：\n",
    "  - HTML是动态加载的（JavaScript渲染）\n",
    "  - 选择器写错了\n",
    "  - 元素在iframe中\n",
    "  - 需要登录才能看到\n",
    "- **解决方法**：\n",
    "  - 检查网页源代码（不是开发者工具中的Elements）\n",
    "  - 使用Selenium处理动态内容\n",
    "  - 仔细检查CSS选择器\n",
    "\n",
    "#### 2. 如何处理编码问题？\n",
    "- **问题**：中文乱码\n",
    "- **解决**：确保requests获取内容时指定正确编码\n",
    "  ```python\n",
    "  response.encoding = 'utf-8'  # 或根据实际情况设置\n",
    "  ```\n",
    "\n",
    "#### 3. 性能优化建议\n",
    "- 使用lxml解析器（比html.parser快）\n",
    "- 使用CSS选择器（比find_all()快）\n",
    "- 避免重复解析同一文档\n",
    "- 对于大量数据，考虑使用多线程（但要控制频率）\n",
    "\n",
    "### 最佳实践总结\n",
    "\n",
    "1. **选择合适的解析器**：生产环境推荐lxml\n",
    "2. **使用CSS选择器**：更简洁、更强大\n",
    "3. **异常处理**：总是处理可能的异常情况\n",
    "4. **代码可读性**：使用有意义的变量名\n",
    "5. **遵守法律法规**：尊重网站规则和法律法规\n",
    "6. **性能考虑**：避免不必要的解析和查找\n",
    "7. **数据验证**：提取数据后验证其有效性\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 最佳实践示例：完整的、健壮的解析函数\n",
    "\n",
    "def robust_html_parser(html_content, selector, default_value=None):\n",
    "    \"\"\"\n",
    "    一个健壮的HTML解析函数，包含错误处理\n",
    "    \n",
    "    参数:\n",
    "        html_content: HTML字符串\n",
    "        selector: CSS选择器\n",
    "        default_value: 如果找不到元素，返回的默认值\n",
    "    \n",
    "    返回:\n",
    "        找到的元素文本，如果找不到返回default_value\n",
    "    \"\"\"\n",
    "    try:\n",
    "        soup = BeautifulSoup(html_content, 'lxml')\n",
    "        element = soup.select_one(selector)\n",
    "        \n",
    "        if element:\n",
    "            return element.get_text(strip=True)\n",
    "        else:\n",
    "            return default_value\n",
    "    except Exception as e:\n",
    "        print(f\"解析错误: {e}\")\n",
    "        return default_value\n",
    "\n",
    "# 使用示例\n",
    "test_html = \"\"\"\n",
    "<div>\n",
    "    <p class=\"content\">这是内容</p>\n",
    "    <span class=\"author\">作者名</span>\n",
    "</div>\n",
    "\"\"\"\n",
    "\n",
    "# 正常情况\n",
    "content = robust_html_parser(test_html, '.content', '未找到内容')\n",
    "print(f\"内容: {content}\")\n",
    "\n",
    "# 找不到元素的情况\n",
    "missing = robust_html_parser(test_html, '.not-exist', '默认值')\n",
    "print(f\"不存在的元素: {missing}\")\n",
    "\n",
    "# 总结\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"学习总结\")\n",
    "print(\"=\"*60)\n",
    "print(\"\"\"\n",
    "通过本教程，你应该已经掌握了：\n",
    "\n",
    "1. ✅ BeautifulSoup和lxml的基本使用方法\n",
    "2. ✅ HTML解析和元素查找的各种技巧\n",
    "3. ✅ 数据提取和清洗的方法\n",
    "4. ✅ CSS选择器的使用\n",
    "5. ✅ HTTP协议基础知识\n",
    "6. ✅ 网络爬虫的安全和合规问题\n",
    "\n",
    "下一步建议：\n",
    "- 练习解析不同类型的HTML结构\n",
    "- 学习处理更复杂的嵌套数据\n",
    "- 了解XPath（lxml的高级功能）\n",
    "- 学习Selenium处理动态网页\n",
    "- 深入学习HTTP协议和网络安全\n",
    "\n",
    "记住：技术是工具，使用时要遵守法律法规和道德规范！\n",
    "\"\"\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
