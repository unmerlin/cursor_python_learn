{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pandasæ•°æ®è¯»å–ä¸ä¿å­˜\n",
    "\n",
    "## ğŸ“š å­¦ä¹ ç›®æ ‡\n",
    "æœ¬æ•™ç¨‹å°†ç³»ç»Ÿè®²è§£pandasçš„æ•°æ®è¯»å–å’Œä¿å­˜åŠŸèƒ½ï¼Œè¿™æ˜¯æ•°æ®åˆ†æçš„ç¬¬ä¸€æ­¥ã€‚\n",
    "\n",
    "### æœ¬æ•™ç¨‹å°†å­¦ä¹ ï¼š\n",
    "1. **è¯»å–CSVæ–‡ä»¶** - read_csvçš„å„ç§å‚æ•°è¯¦è§£\n",
    "2. **è¯»å–Excelæ–‡ä»¶** - read_excelçš„ä½¿ç”¨æ–¹æ³•\n",
    "3. **è¯»å–å…¶ä»–æ ¼å¼** - JSONã€HTMLã€SQLç­‰\n",
    "4. **æ•°æ®ä¿å­˜** - to_csvã€to_excelã€to_jsonç­‰\n",
    "5. **å¤§æ–‡ä»¶å¤„ç†** - chunksizeå‚æ•°çš„ä½¿ç”¨\n",
    "6. **ç¼–ç é—®é¢˜** - encodingå‚æ•°çš„å¤„ç†\n",
    "\n",
    "### å­¦ä¹ é‡ç‚¹ï¼š\n",
    "- æŒæ¡å¸¸ç”¨å‚æ•°çš„ç”¨æ³•\n",
    "- ç†è§£å¸¸è§é”™è¯¯å’Œè§£å†³æ–¹æ³•\n",
    "- å­¦ä¼šå¤„ç†å¤§æ–‡ä»¶å’Œç¼–ç é—®é¢˜"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“¦ å¯¼å…¥å¿…è¦çš„åº“"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"åº“å¯¼å…¥æˆåŠŸï¼\")\n",
    "print(f\"pandasç‰ˆæœ¬: {pd.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# ç¬¬ä¸€éƒ¨åˆ†ï¼šè¯»å–CSVæ–‡ä»¶\n",
    "\n",
    "## â­ read_csv() - æœ€å¸¸ç”¨çš„æ•°æ®è¯»å–æ–¹æ³•\n",
    "\n",
    "CSVï¼ˆComma-Separated Valuesï¼‰æ˜¯æœ€å¸¸è§çš„æ•°æ®æ ¼å¼ï¼Œpandasçš„`read_csv()`åŠŸèƒ½éå¸¸å¼ºå¤§ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. åŸºæœ¬è¯»å–CSVæ–‡ä»¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# é¦–å…ˆåˆ›å»ºä¸€ä¸ªç¤ºä¾‹CSVæ–‡ä»¶\n",
    "data = {\n",
    "    'å§“å': ['å¼ ä¸‰', 'æå››', 'ç‹äº”'],\n",
    "    'å¹´é¾„': [25, 30, 28],\n",
    "    'åŸå¸‚': ['åŒ—äº¬', 'ä¸Šæµ·', 'å¹¿å·']\n",
    "}\n",
    "df_sample = pd.DataFrame(data)\n",
    "df_sample.to_csv('sample.csv', index=False, encoding='utf-8-sig')\n",
    "print(\"å·²åˆ›å»ºç¤ºä¾‹CSVæ–‡ä»¶: sample.csv\")\n",
    "\n",
    "# åŸºæœ¬è¯»å–æ–¹æ³•\n",
    "df = pd.read_csv('sample.csv')\n",
    "print(\"\\nè¯»å–çš„æ•°æ®ï¼š\")\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. encodingå‚æ•° - å¤„ç†ä¸­æ–‡ç¼–ç é—®é¢˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¸¸è§ç¼–ç é—®é¢˜ï¼šä¸­æ–‡ä¹±ç \n",
    "# è§£å†³æ–¹æ¡ˆï¼šæŒ‡å®šæ­£ç¡®çš„ç¼–ç \n",
    "\n",
    "# UTF-8ç¼–ç ï¼ˆæ¨èï¼Œæ”¯æŒä¸­æ–‡ï¼‰\n",
    "df_utf8 = pd.read_csv('sample.csv', encoding='utf-8-sig')\n",
    "print(\"ä½¿ç”¨utf-8-sigç¼–ç è¯»å–ï¼š\")\n",
    "print(df_utf8)\n",
    "\n",
    "# å…¶ä»–å¸¸è§ç¼–ç ï¼š\n",
    "# - 'utf-8': æ ‡å‡†UTF-8\n",
    "# - 'utf-8-sig': å¸¦BOMçš„UTF-8ï¼ˆExcelå¯¼å‡ºå¸¸ç”¨ï¼‰\n",
    "# - 'gbk'æˆ–'gb2312': ä¸­æ–‡Windowsç³»ç»Ÿå¸¸ç”¨\n",
    "# - 'latin-1': è¥¿æ¬§å­—ç¬¦"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. sepå‚æ•° - æŒ‡å®šåˆ†éš”ç¬¦"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# é»˜è®¤åˆ†éš”ç¬¦æ˜¯é€—å·ï¼Œä½†å¯ä»¥æŒ‡å®šå…¶ä»–åˆ†éš”ç¬¦\n",
    "\n",
    "# åˆ›å»ºä½¿ç”¨åˆ†å·åˆ†éš”çš„CSV\n",
    "df_sample.to_csv('sample_semicolon.csv', index=False, sep=';', encoding='utf-8-sig')\n",
    "\n",
    "# è¯»å–æ—¶æŒ‡å®šåˆ†éš”ç¬¦\n",
    "df_semicolon = pd.read_csv('sample_semicolon.csv', sep=';', encoding='utf-8-sig')\n",
    "print(\"ä½¿ç”¨åˆ†å·åˆ†éš”ç¬¦è¯»å–ï¼š\")\n",
    "print(df_semicolon)\n",
    "\n",
    "# å¸¸è§åˆ†éš”ç¬¦ï¼š\n",
    "# - ',': é€—å·ï¼ˆé»˜è®¤ï¼‰\n",
    "# - ';': åˆ†å·\n",
    "# - '\\t': åˆ¶è¡¨ç¬¦ï¼ˆTSVæ–‡ä»¶ï¼‰\n",
    "# - '|': ç«–çº¿"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. headerå‚æ•° - æŒ‡å®šè¡¨å¤´è¡Œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åˆ›å»ºæ²¡æœ‰è¡¨å¤´çš„CSVæ–‡ä»¶\n",
    "df_sample.to_csv('sample_no_header.csv', index=False, header=False, encoding='utf-8-sig')\n",
    "\n",
    "# è¯»å–æ—¶æŒ‡å®šåˆ—å\n",
    "df_no_header = pd.read_csv('sample_no_header.csv', \n",
    "                            header=None, \n",
    "                            names=['å§“å', 'å¹´é¾„', 'åŸå¸‚'],\n",
    "                            encoding='utf-8-sig')\n",
    "print(\"è¯»å–æ— è¡¨å¤´æ–‡ä»¶å¹¶æŒ‡å®šåˆ—åï¼š\")\n",
    "print(df_no_header)\n",
    "\n",
    "# headerå‚æ•°è¯´æ˜ï¼š\n",
    "# - é»˜è®¤0ï¼šç¬¬ä¸€è¡Œä½œä¸ºåˆ—å\n",
    "# - Noneï¼šæ²¡æœ‰åˆ—åï¼Œéœ€è¦æ‰‹åŠ¨æŒ‡å®š\n",
    "# - æ•°å­—ï¼šæŒ‡å®šç¬¬å‡ è¡Œä½œä¸ºåˆ—å"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. index_colå‚æ•° - æŒ‡å®šç´¢å¼•åˆ—"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åˆ›å»ºå¸¦ç´¢å¼•çš„CSVæ–‡ä»¶\n",
    "df_sample.index = ['å‘˜å·¥1', 'å‘˜å·¥2', 'å‘˜å·¥3']\n",
    "df_sample.to_csv('sample_with_index.csv', encoding='utf-8-sig')\n",
    "\n",
    "# è¯»å–æ—¶æŒ‡å®šç´¢å¼•åˆ—\n",
    "df_with_index = pd.read_csv('sample_with_index.csv', \n",
    "                            index_col=0,  # ç¬¬ä¸€åˆ—ä½œä¸ºç´¢å¼•\n",
    "                            encoding='utf-8-sig')\n",
    "print(\"è¯»å–å¹¶æŒ‡å®šç´¢å¼•åˆ—ï¼š\")\n",
    "print(df_with_index)\n",
    "print(f\"\\nç´¢å¼•: {df_with_index.index}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. usecolså‚æ•° - åªè¯»å–æŒ‡å®šåˆ—"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åªè¯»å–æŒ‡å®šçš„åˆ—ï¼ˆèŠ‚çœå†…å­˜ï¼‰\n",
    "df_selected = pd.read_csv('sample.csv', \n",
    "                         usecols=['å§“å', 'å¹´é¾„'],  # åªè¯»å–è¿™ä¸¤åˆ—\n",
    "                         encoding='utf-8-sig')\n",
    "print(\"åªè¯»å–æŒ‡å®šåˆ—ï¼š\")\n",
    "print(df_selected)\n",
    "\n",
    "# usecolsä¹Ÿå¯ä»¥ä½¿ç”¨åˆ—çš„ä½ç½®ï¼ˆä»0å¼€å§‹ï¼‰\n",
    "df_by_position = pd.read_csv('sample.csv', \n",
    "                              usecols=[0, 1],  # ç¬¬1åˆ—å’Œç¬¬2åˆ—\n",
    "                              encoding='utf-8-sig')\n",
    "print(\"\\næŒ‰ä½ç½®é€‰æ‹©åˆ—ï¼š\")\n",
    "print(df_by_position)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. nrowså‚æ•° - åªè¯»å–å‰Nè¡Œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åªè¯»å–å‰Nè¡Œï¼ˆç”¨äºå¿«é€Ÿé¢„è§ˆå¤§æ–‡ä»¶ï¼‰\n",
    "df_preview = pd.read_csv('sample.csv', \n",
    "                        nrows=2,  # åªè¯»å–å‰2è¡Œ\n",
    "                        encoding='utf-8-sig')\n",
    "print(\"åªè¯»å–å‰2è¡Œï¼ˆé¢„è§ˆï¼‰ï¼š\")\n",
    "print(df_preview)\n",
    "\n",
    "# å®é™…åº”ç”¨ï¼šå¿«é€ŸæŸ¥çœ‹å¤§æ–‡ä»¶çš„ç»“æ„ï¼Œä¸éœ€è¦è¯»å–å…¨éƒ¨æ•°æ®"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. skiprowså’Œskipfooterå‚æ•° - è·³è¿‡è¡Œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# è·³è¿‡æ–‡ä»¶å¼€å¤´çš„è¡Œï¼ˆå¦‚æ³¨é‡Šã€è¯´æ˜ç­‰ï¼‰\n",
    "# åˆ›å»ºå¸¦æ³¨é‡Šçš„CSVæ–‡ä»¶\n",
    "with open('sample_with_comments.csv', 'w', encoding='utf-8-sig') as f:\n",
    "    f.write('# è¿™æ˜¯æ³¨é‡Šè¡Œ\\n')\n",
    "    f.write('# æ•°æ®å¼€å§‹\\n')\n",
    "    df_sample.to_csv(f, index=False)\n",
    "\n",
    "# è·³è¿‡å‰2è¡Œï¼ˆæ³¨é‡Šè¡Œï¼‰\n",
    "df_skip = pd.read_csv('sample_with_comments.csv', \n",
    "                      skiprows=2,  # è·³è¿‡å‰2è¡Œ\n",
    "                      encoding='utf-8-sig')\n",
    "print(\"è·³è¿‡å‰2è¡Œåè¯»å–ï¼š\")\n",
    "print(df_skip)\n",
    "\n",
    "# skipfooter: è·³è¿‡æ–‡ä»¶æœ«å°¾çš„è¡Œ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. na_valueså‚æ•° - æŒ‡å®šç¼ºå¤±å€¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åˆ›å»ºåŒ…å«ç‰¹æ®Šç¼ºå¤±å€¼æ ‡è®°çš„CSV\n",
    "data_with_na = {\n",
    "    'å§“å': ['å¼ ä¸‰', 'æå››', 'ç‹äº”'],\n",
    "    'å¹´é¾„': [25, 'N/A', 28],\n",
    "    'åŸå¸‚': ['åŒ—äº¬', 'ä¸Šæµ·', 'NULL']\n",
    "}\n",
    "df_na = pd.DataFrame(data_with_na)\n",
    "df_na.to_csv('sample_with_na.csv', index=False, encoding='utf-8-sig')\n",
    "\n",
    "# æŒ‡å®šå“ªäº›å€¼åº”è¯¥è¢«è§†ä¸ºç¼ºå¤±å€¼\n",
    "df_na_read = pd.read_csv('sample_with_na.csv', \n",
    "                         na_values=['N/A', 'NULL', ''],  # è¿™äº›å€¼ä¼šè¢«è¯†åˆ«ä¸ºNaN\n",
    "                         encoding='utf-8-sig')\n",
    "print(\"è¯»å–å¹¶è¯†åˆ«ç¼ºå¤±å€¼ï¼š\")\n",
    "print(df_na_read)\n",
    "print(\"\\nç¼ºå¤±å€¼ç»Ÿè®¡ï¼š\")\n",
    "print(df_na_read.isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. dtypeå‚æ•° - æŒ‡å®šæ•°æ®ç±»å‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# è¯»å–æ—¶æŒ‡å®šæ•°æ®ç±»å‹ï¼ˆæé«˜æ€§èƒ½ï¼Œé¿å…ç±»å‹æ¨æ–­ï¼‰\n",
    "df_typed = pd.read_csv('sample.csv', \n",
    "                       dtype={'å¹´é¾„': 'int32', 'å§“å': 'string'},  # æŒ‡å®šæ•°æ®ç±»å‹\n",
    "                       encoding='utf-8-sig')\n",
    "print(\"æŒ‡å®šæ•°æ®ç±»å‹åè¯»å–ï¼š\")\n",
    "print(df_typed)\n",
    "print(\"\\næ•°æ®ç±»å‹ï¼š\")\n",
    "print(df_typed.dtypes)\n",
    "\n",
    "# ä¼˜åŠ¿ï¼š\n",
    "# - æé«˜è¯»å–é€Ÿåº¦\n",
    "# - èŠ‚çœå†…å­˜\n",
    "# - é¿å…ç±»å‹æ¨æ–­é”™è¯¯"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# ç¬¬äºŒéƒ¨åˆ†ï¼šè¯»å–Excelæ–‡ä»¶\n",
    "\n",
    "## â­ read_excel() - Excelæ–‡ä»¶è¯»å–"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. åŸºæœ¬è¯»å–Excelæ–‡ä»¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# é¦–å…ˆåˆ›å»ºç¤ºä¾‹Excelæ–‡ä»¶\n",
    "df_sample.to_excel('sample.xlsx', index=False)\n",
    "print(\"å·²åˆ›å»ºç¤ºä¾‹Excelæ–‡ä»¶: sample.xlsx\")\n",
    "\n",
    "# åŸºæœ¬è¯»å–æ–¹æ³•ï¼ˆéœ€è¦å®‰è£…openpyxl: pip install openpyxlï¼‰\n",
    "try:\n",
    "    df_excel = pd.read_excel('sample.xlsx')\n",
    "    print(\"\\nä»Excelè¯»å–çš„æ•°æ®ï¼š\")\n",
    "    print(df_excel)\n",
    "except ImportError:\n",
    "    print(\"\\néœ€è¦å®‰è£…openpyxlåº“: pip install openpyxl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. sheet_nameå‚æ•° - æŒ‡å®šå·¥ä½œè¡¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åˆ›å»ºåŒ…å«å¤šä¸ªå·¥ä½œè¡¨çš„Excelæ–‡ä»¶\n",
    "with pd.ExcelWriter('sample_multi_sheet.xlsx') as writer:\n",
    "    df_sample.to_excel(writer, sheet_name='Sheet1', index=False)\n",
    "    df_sample.to_excel(writer, sheet_name='Sheet2', index=False)\n",
    "\n",
    "print(\"å·²åˆ›å»ºå¤šå·¥ä½œè¡¨Excelæ–‡ä»¶\")\n",
    "\n",
    "# è¯»å–æŒ‡å®šå·¥ä½œè¡¨\n",
    "try:\n",
    "    df_sheet1 = pd.read_excel('sample_multi_sheet.xlsx', sheet_name='Sheet1')\n",
    "    print(\"\\nè¯»å–Sheet1ï¼š\")\n",
    "    print(df_sheet1)\n",
    "    \n",
    "    # è¯»å–æ‰€æœ‰å·¥ä½œè¡¨ï¼ˆè¿”å›å­—å…¸ï¼‰\n",
    "    all_sheets = pd.read_excel('sample_multi_sheet.xlsx', sheet_name=None)\n",
    "    print(\"\\næ‰€æœ‰å·¥ä½œè¡¨ï¼š\")\n",
    "    for sheet_name, df in all_sheets.items():\n",
    "        print(f\"\\n{sheet_name}:\")\n",
    "        print(df)\n",
    "except ImportError:\n",
    "    print(\"éœ€è¦å®‰è£…openpyxlåº“\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. headerå’Œindex_colå‚æ•° - Excelä¸­çš„ä½¿ç”¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Excelä¸­çš„headerå’Œindex_colç”¨æ³•ä¸CSVç±»ä¼¼\n",
    "try:\n",
    "    # æŒ‡å®šè¡¨å¤´è¡Œï¼ˆé»˜è®¤ç¬¬ä¸€è¡Œï¼‰\n",
    "    df_excel_header = pd.read_excel('sample.xlsx', header=0)\n",
    "    \n",
    "    # æŒ‡å®šç´¢å¼•åˆ—\n",
    "    df_sample.index = ['A', 'B', 'C']\n",
    "    df_sample.to_excel('sample_with_index.xlsx')\n",
    "    df_excel_index = pd.read_excel('sample_with_index.xlsx', index_col=0)\n",
    "    \n",
    "    print(\"æŒ‡å®šç´¢å¼•åˆ—è¯»å–ï¼š\")\n",
    "    print(df_excel_index)\n",
    "except ImportError:\n",
    "    print(\"éœ€è¦å®‰è£…openpyxlåº“\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# ç¬¬ä¸‰éƒ¨åˆ†ï¼šè¯»å–å…¶ä»–æ•°æ®æº\n",
    "\n",
    "## â­ JSONã€HTMLã€SQLç­‰æ ¼å¼"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. è¯»å–JSONæ–‡ä»¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åˆ›å»ºç¤ºä¾‹JSONæ–‡ä»¶\n",
    "import json\n",
    "\n",
    "json_data = [\n",
    "    {'å§“å': 'å¼ ä¸‰', 'å¹´é¾„': 25, 'åŸå¸‚': 'åŒ—äº¬'},\n",
    "    {'å§“å': 'æå››', 'å¹´é¾„': 30, 'åŸå¸‚': 'ä¸Šæµ·'},\n",
    "    {'å§“å': 'ç‹äº”', 'å¹´é¾„': 28, 'åŸå¸‚': 'å¹¿å·'}\n",
    "]\n",
    "\n",
    "with open('sample.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(json_data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"å·²åˆ›å»ºç¤ºä¾‹JSONæ–‡ä»¶\")\n",
    "\n",
    "# è¯»å–JSONæ–‡ä»¶\n",
    "df_json = pd.read_json('sample.json')\n",
    "print(\"\\nä»JSONè¯»å–çš„æ•°æ®ï¼š\")\n",
    "print(df_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. è¯»å–HTMLè¡¨æ ¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# è¯»å–HTMLä¸­çš„è¡¨æ ¼ï¼ˆéœ€è¦å®‰è£…lxmlå’Œhtml5libï¼‰\n",
    "# æ³¨æ„ï¼šread_htmlè¿”å›DataFrameåˆ—è¡¨ï¼ˆå¯èƒ½æœ‰å¤šä¸ªè¡¨æ ¼ï¼‰\n",
    "\n",
    "html_content = \"\"\"\n",
    "<table>\n",
    "  <tr><th>å§“å</th><th>å¹´é¾„</th><th>åŸå¸‚</th></tr>\n",
    "  <tr><td>å¼ ä¸‰</td><td>25</td><td>åŒ—äº¬</td></tr>\n",
    "  <tr><td>æå››</td><td>30</td><td>ä¸Šæµ·</td></tr>\n",
    "</table>\n",
    "\"\"\"\n",
    "\n",
    "with open('sample.html', 'w', encoding='utf-8') as f:\n",
    "    f.write(html_content)\n",
    "\n",
    "try:\n",
    "    # è¯»å–HTMLè¡¨æ ¼\n",
    "    dfs_html = pd.read_html('sample.html')\n",
    "    print(f\"æ‰¾åˆ° {len(dfs_html)} ä¸ªè¡¨æ ¼\")\n",
    "    print(\"\\nç¬¬ä¸€ä¸ªè¡¨æ ¼ï¼š\")\n",
    "    print(dfs_html[0])\n",
    "except ImportError:\n",
    "    print(\"éœ€è¦å®‰è£…lxmlæˆ–html5lib: pip install lxml html5lib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. è¯»å–SQLæ•°æ®åº“"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# è¯»å–SQLæ•°æ®åº“ï¼ˆéœ€è¦å®‰è£…ç›¸åº”çš„æ•°æ®åº“é©±åŠ¨ï¼‰\n",
    "# ç¤ºä¾‹ï¼šSQLiteæ•°æ®åº“\n",
    "\n",
    "import sqlite3\n",
    "\n",
    "# åˆ›å»ºSQLiteæ•°æ®åº“å’Œè¡¨\n",
    "conn = sqlite3.connect('sample.db')\n",
    "df_sample.to_sql('employees', conn, if_exists='replace', index=False)\n",
    "conn.close()\n",
    "\n",
    "print(\"å·²åˆ›å»ºSQLiteæ•°æ®åº“\")\n",
    "\n",
    "# ä»SQLè¯»å–æ•°æ®\n",
    "conn = sqlite3.connect('sample.db')\n",
    "df_sql = pd.read_sql('SELECT * FROM employees', conn)\n",
    "conn.close()\n",
    "\n",
    "print(\"\\nä»SQLè¯»å–çš„æ•°æ®ï¼š\")\n",
    "print(df_sql)\n",
    "\n",
    "# å…¶ä»–æ•°æ®åº“ï¼š\n",
    "# - MySQL: éœ€è¦pymysqlæˆ–mysql-connector-python\n",
    "# - PostgreSQL: éœ€è¦psycopg2\n",
    "# - SQL Server: éœ€è¦pyodbc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# ç¬¬å››éƒ¨åˆ†ï¼šæ•°æ®ä¿å­˜\n",
    "\n",
    "## â­ ä¿å­˜æ•°æ®åˆ°å„ç§æ ¼å¼"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. ä¿å­˜ä¸ºCSVæ–‡ä»¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åŸºæœ¬ä¿å­˜\n",
    "df_sample.to_csv('output.csv', index=False, encoding='utf-8-sig')\n",
    "print(\"å·²ä¿å­˜ä¸ºCSVæ–‡ä»¶\")\n",
    "\n",
    "# å¸¸ç”¨å‚æ•°ï¼š\n",
    "# - index=False: ä¸ä¿å­˜ç´¢å¼•\n",
    "# - encoding='utf-8-sig': æ”¯æŒä¸­æ–‡ï¼ˆExcelå¯è¯»ï¼‰\n",
    "# - sep=',': åˆ†éš”ç¬¦ï¼ˆé»˜è®¤é€—å·ï¼‰\n",
    "# - na_rep='': ç¼ºå¤±å€¼è¡¨ç¤ºï¼ˆé»˜è®¤ç©ºå­—ç¬¦ä¸²ï¼‰"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. ä¿å­˜ä¸ºExcelæ–‡ä»¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åŸºæœ¬ä¿å­˜ï¼ˆéœ€è¦openpyxlï¼‰\n",
    "try:\n",
    "    df_sample.to_excel('output.xlsx', index=False)\n",
    "    print(\"å·²ä¿å­˜ä¸ºExcelæ–‡ä»¶\")\n",
    "    \n",
    "    # ä¿å­˜åˆ°å¤šä¸ªå·¥ä½œè¡¨\n",
    "    with pd.ExcelWriter('output_multi.xlsx') as writer:\n",
    "        df_sample.to_excel(writer, sheet_name='æ•°æ®1', index=False)\n",
    "        df_sample.to_excel(writer, sheet_name='æ•°æ®2', index=False)\n",
    "    print(\"å·²ä¿å­˜ä¸ºå¤šå·¥ä½œè¡¨Excelæ–‡ä»¶\")\n",
    "except ImportError:\n",
    "    print(\"éœ€è¦å®‰è£…openpyxlåº“\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. ä¿å­˜ä¸ºJSONæ–‡ä»¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ä¿å­˜ä¸ºJSON\n",
    "df_sample.to_json('output.json', orient='records', force_ascii=False, indent=2)\n",
    "print(\"å·²ä¿å­˜ä¸ºJSONæ–‡ä»¶\")\n",
    "\n",
    "# orientå‚æ•°è¯´æ˜ï¼š\n",
    "# - 'records': æ¯è¡Œä¸€ä¸ªJSONå¯¹è±¡ï¼ˆå¸¸ç”¨ï¼‰\n",
    "# - 'index': ä»¥ç´¢å¼•ä¸ºé”®\n",
    "# - 'values': åªä¿å­˜å€¼\n",
    "# - 'table': è¡¨æ ¼æ ¼å¼"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# ç¬¬äº”éƒ¨åˆ†ï¼šå¤„ç†å¤§æ–‡ä»¶\n",
    "\n",
    "## â­ chunksizeå‚æ•° - åˆ†å—è¯»å–å¤§æ–‡ä»¶"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. ä½¿ç”¨chunksizeåˆ†å—è¯»å–"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åˆ›å»ºä¸€ä¸ªå¤§æ–‡ä»¶ç”¨äºæ¼”ç¤º\n",
    "large_data = pd.DataFrame({\n",
    "    'ID': range(1, 1001),\n",
    "    'å€¼': np.random.randn(1000)\n",
    "})\n",
    "large_data.to_csv('large_file.csv', index=False, encoding='utf-8-sig')\n",
    "print(\"å·²åˆ›å»ºå¤§æ–‡ä»¶ï¼ˆ1000è¡Œï¼‰\")\n",
    "\n",
    "# åˆ†å—è¯»å–ï¼ˆæ¯æ¬¡è¯»å–100è¡Œï¼‰\n",
    "chunk_size = 100\n",
    "chunks = pd.read_csv('large_file.csv', chunksize=chunk_size, encoding='utf-8-sig')\n",
    "\n",
    "print(f\"\\nåˆ†å—è¯»å–ï¼ˆæ¯å—{chunk_size}è¡Œï¼‰ï¼š\")\n",
    "for i, chunk in enumerate(chunks, 1):\n",
    "    print(f\"ç¬¬{i}å—: {len(chunk)}è¡Œ\")\n",
    "    if i >= 3:  # åªæ˜¾ç¤ºå‰3å—\n",
    "        print(\"...\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. åˆ†å—å¤„ç†å¤§æ–‡ä»¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å®é™…åº”ç”¨ï¼šåˆ†å—å¤„ç†å¤§æ–‡ä»¶å¹¶æ±‡æ€»ç»“æœ\n",
    "chunks = pd.read_csv('large_file.csv', chunksize=100, encoding='utf-8-sig')\n",
    "\n",
    "total_rows = 0\n",
    "total_sum = 0\n",
    "\n",
    "for chunk in chunks:\n",
    "    total_rows += len(chunk)\n",
    "    total_sum += chunk['å€¼'].sum()\n",
    "\n",
    "print(f\"æ€»è¡Œæ•°: {total_rows}\")\n",
    "print(f\"å€¼çš„æ€»å’Œ: {total_sum:.2f}\")\n",
    "\n",
    "# ä¼˜åŠ¿ï¼š\n",
    "# - å†…å­˜å ç”¨å°\n",
    "# - å¯ä»¥å¤„ç†è¶…å¤§æ–‡ä»¶\n",
    "# - å¯ä»¥é€å—å¤„ç†æ•°æ®"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# ç¬¬å…­éƒ¨åˆ†ï¼šå¸¸è§é”™è¯¯å¤„ç†\n",
    "\n",
    "## â­ ç¼–ç é”™è¯¯å¤„ç†"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. å¤„ç†ç¼–ç é”™è¯¯"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¸¸è§é”™è¯¯ï¼šUnicodeDecodeError\n",
    "# è§£å†³æ–¹æ¡ˆï¼šå°è¯•ä¸åŒçš„ç¼–ç \n",
    "\n",
    "encodings = ['utf-8', 'utf-8-sig', 'gbk', 'gb2312', 'latin-1']\n",
    "\n",
    "def read_with_encoding(file_path, encodings):\n",
    "    \"\"\"å°è¯•ä¸åŒç¼–ç è¯»å–æ–‡ä»¶\"\"\"\n",
    "    for encoding in encodings:\n",
    "        try:\n",
    "            df = pd.read_csv(file_path, encoding=encoding)\n",
    "            print(f\"æˆåŠŸä½¿ç”¨ {encoding} ç¼–ç è¯»å–\")\n",
    "            return df\n",
    "        except UnicodeDecodeError:\n",
    "            continue\n",
    "    raise ValueError(\"æ— æ³•ä½¿ç”¨ä»»ä½•ç¼–ç è¯»å–æ–‡ä»¶\")\n",
    "\n",
    "# ä½¿ç”¨ç¤ºä¾‹\n",
    "df_auto = read_with_encoding('sample.csv', encodings)\n",
    "print(\"\\nè¯»å–çš„æ•°æ®ï¼š\")\n",
    "print(df_auto.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. å¤„ç†æ–‡ä»¶ä¸å­˜åœ¨é”™è¯¯"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¤„ç†æ–‡ä»¶ä¸å­˜åœ¨çš„é”™è¯¯\n",
    "import os\n",
    "\n",
    "def safe_read_csv(file_path, **kwargs):\n",
    "    \"\"\"å®‰å…¨è¯»å–CSVæ–‡ä»¶\"\"\"\n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}\")\n",
    "    \n",
    "    try:\n",
    "        return pd.read_csv(file_path, **kwargs)\n",
    "    except Exception as e:\n",
    "        print(f\"è¯»å–æ–‡ä»¶æ—¶å‡ºé”™: {e}\")\n",
    "        raise\n",
    "\n",
    "# ä½¿ç”¨ç¤ºä¾‹\n",
    "try:\n",
    "    df_safe = safe_read_csv('sample.csv', encoding='utf-8-sig')\n",
    "    print(\"æ–‡ä»¶è¯»å–æˆåŠŸ\")\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"é”™è¯¯: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. å¤„ç†æ•°æ®ç±»å‹é”™è¯¯"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¤„ç†æ•°æ®ç±»å‹è½¬æ¢é”™è¯¯\n",
    "# åˆ›å»ºåŒ…å«é”™è¯¯æ•°æ®çš„CSV\n",
    "data_error = {\n",
    "    'ID': ['1', '2', 'abc', '4'],  # åŒ…å«éæ•°å­—\n",
    "    'å¹´é¾„': [25, 30, 28, 35]\n",
    "}\n",
    "df_error = pd.DataFrame(data_error)\n",
    "df_error.to_csv('sample_error.csv', index=False, encoding='utf-8-sig')\n",
    "\n",
    "# è¯»å–æ—¶å¤„ç†é”™è¯¯\n",
    "df_error_read = pd.read_csv('sample_error.csv', \n",
    "                            encoding='utf-8-sig',\n",
    "                            dtype={'ID': 'string'})  # ä½¿ç”¨å­—ç¬¦ä¸²ç±»å‹é¿å…é”™è¯¯\n",
    "\n",
    "print(\"å¤„ç†é”™è¯¯æ•°æ®ï¼š\")\n",
    "print(df_error_read)\n",
    "print(\"\\næ•°æ®ç±»å‹ï¼š\")\n",
    "print(df_error_read.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# æ€»ç»“\n",
    "\n",
    "## ğŸ¯ æœ¬æ•™ç¨‹é‡ç‚¹å›é¡¾\n",
    "\n",
    "### 1. CSVæ–‡ä»¶è¯»å–\n",
    "- `encoding`: å¤„ç†ä¸­æ–‡ç¼–ç \n",
    "- `sep`: æŒ‡å®šåˆ†éš”ç¬¦\n",
    "- `header`: æŒ‡å®šè¡¨å¤´è¡Œ\n",
    "- `index_col`: æŒ‡å®šç´¢å¼•åˆ—\n",
    "- `usecols`: é€‰æ‹©ç‰¹å®šåˆ—\n",
    "- `nrows`: åªè¯»å–å‰Nè¡Œ\n",
    "- `chunksize`: åˆ†å—è¯»å–å¤§æ–‡ä»¶\n",
    "\n",
    "### 2. Excelæ–‡ä»¶è¯»å–\n",
    "- `sheet_name`: æŒ‡å®šå·¥ä½œè¡¨\n",
    "- éœ€è¦å®‰è£…openpyxlåº“\n",
    "\n",
    "### 3. å…¶ä»–æ•°æ®æº\n",
    "- JSON: `read_json()`\n",
    "- HTML: `read_html()`\n",
    "- SQL: `read_sql()`\n",
    "\n",
    "### 4. æ•°æ®ä¿å­˜\n",
    "- CSV: `to_csv()`\n",
    "- Excel: `to_excel()`\n",
    "- JSON: `to_json()`\n",
    "\n",
    "### 5. å¤§æ–‡ä»¶å¤„ç†\n",
    "- ä½¿ç”¨`chunksize`å‚æ•°åˆ†å—è¯»å–\n",
    "- é€å—å¤„ç†æ•°æ®\n",
    "\n",
    "### 6. é”™è¯¯å¤„ç†\n",
    "- ç¼–ç é”™è¯¯ï¼šå°è¯•ä¸åŒç¼–ç \n",
    "- æ–‡ä»¶ä¸å­˜åœ¨ï¼šæ£€æŸ¥æ–‡ä»¶è·¯å¾„\n",
    "- æ•°æ®ç±»å‹é”™è¯¯ï¼šæŒ‡å®šæ­£ç¡®çš„dtype\n",
    "\n",
    "## ğŸ“š ä¸‹ä¸€æ­¥å­¦ä¹ \n",
    "\n",
    "æŒæ¡äº†æ•°æ®è¯»å–å’Œä¿å­˜åï¼Œæ¥ä¸‹æ¥å¯ä»¥å­¦ä¹ ï¼š\n",
    "- æ•°æ®æŸ¥çœ‹ä¸åŸºæœ¬ä¿¡æ¯\n",
    "- ç´¢å¼•ä¸é€‰æ‹©æ•°æ®\n",
    "- æ•°æ®ç­›é€‰ä¸è¿‡æ»¤\n",
    "\n",
    "---\n",
    "\n",
    "**è®°ä½ï¼šæ•°æ®è¯»å–æ˜¯æ•°æ®åˆ†æçš„ç¬¬ä¸€æ­¥ï¼Œé€‰æ‹©åˆé€‚çš„å‚æ•°å¯ä»¥å¤§å¤§æé«˜æ•ˆç‡ï¼**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## æ¸…ç†æµ‹è¯•æ–‡ä»¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ¸…ç†æœ¬æ•™ç¨‹åˆ›å»ºçš„æ‰€æœ‰æµ‹è¯•æ–‡ä»¶\n",
    "import os\n",
    "import glob\n",
    "\n",
    "test_files = [\n",
    "    'sample.csv',\n",
    "    'sample_semicolon.csv',\n",
    "    'sample_no_header.csv',\n",
    "    'sample_with_index.csv',\n",
    "    'sample_with_comments.csv',\n",
    "    'sample_with_na.csv',\n",
    "    'sample.xlsx',\n",
    "    'sample_multi_sheet.xlsx',\n",
    "    'sample_with_index.xlsx',\n",
    "    'sample.json',\n",
    "    'sample.html',\n",
    "    'sample.db',\n",
    "    'output.csv',\n",
    "    'output.xlsx',\n",
    "    'output_multi.xlsx',\n",
    "    'output.json',\n",
    "    'large_file.csv',\n",
    "    'sample_error.csv'\n",
    "]\n",
    "\n",
    "for file in test_files:\n",
    "    if os.path.exists(file):\n",
    "        os.remove(file)\n",
    "        print(f\"å·²åˆ é™¤: {file}\")\n",
    "\n",
    "print(\"\\næ¸…ç†å®Œæˆï¼\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
